# -*- coding: utf-8 -*-
"""ESE6450.JOHN.InstructPix2Pix-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PrJNUHY8-jsVTgpTS_hmr6jHSDPegv5g

# Text-Driven Image Editing with InstructPix2Pix
## Final Project: Deep Generative Models (ESE 6450)

By John Harshith Kavuturu (PennID: 67055516)

## 1. Setup and Installation
"""

from google.colab import drive
drive.mount('/content/drive')

# Installing required packages
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install -q diffusers transformers accelerate
!pip install -q k-diffusion omegaconf einops
!pip install -q pillow opencv-python
!pip install -q torchmetrics clip-score
!pip install -q lpips
!pip install -q tqdm pandas matplotlib

import sys
import os
print("Installation complete!")

# Uninstalling the conflicting numpy version
!pip uninstall -y numpy

# Installing a specific stable version of Numpy
!pip install numpy==1.26.4

# Downgrading OpenCV to a version that supports Numpy 1.x
!pip install "opencv-python-headless<4.10" "opencv-python<4.10"

# Re-installing Numba/TF dependencies
!pip install --upgrade --force-reinstall numba tensorflow

# Forcing uninstall Numpy 2.x
!pip uninstall -y numpy

# Installing the standard Numpy 1.x used for Generative AI
!pip install "numpy==1.26.4"

# Forcing install an OpenCV version that does not require Numpy 2.0
!pip install "opencv-python-headless<4.10"

print("Fix installed!")

import numpy
import cv2
import torch

print(f"Numpy version: {numpy.__version__}")
print(f"OpenCV version: {cv2.__version__}")
print(f"Torch version: {torch.__version__}")

import os
import sys

# Cloning PnPInversion repository
if not os.path.exists('/content/PnPInversion'):
    !git clone https://github.com/cure-lab/PnPInversion /content/PnPInversion

# Adding to path
sys.path.append('/content/PnPInversion')
sys.path.append('/content/PnPInversion/models/instructpix2pix/stable_diffusion')

# Creating directories
os.makedirs('/content/checkpoints', exist_ok=True)
os.makedirs('/content/data', exist_ok=True)
os.makedirs('/content/output', exist_ok=True)
os.makedirs('/content/results', exist_ok=True)

print("Setup complete!")

"""## 2. Download Checkpoints and Dataset

"""

# Downloading InstructPix2Pix checkpoint
checkpoint_url = "https://huggingface.co/timbrooks/instruct-pix2pix/resolve/main/instruct-pix2pix-00-22000.ckpt"
checkpoint_path = "/content/checkpoints/instruct-pix2pix-00-22000.ckpt"

if not os.path.exists(checkpoint_path):
    print("Downloading InstructPix2Pix checkpoint...")
    !wget -q --show-progress -O {checkpoint_path} {checkpoint_url}
    print("Checkpoint downloaded!")
else:
    print("Checkpoint already exists!")

# Setup PIE-Bench dataset
pie_bench_path = "/content/PIE-Bench_v1"

if os.path.exists('/content/drive/MyDrive/PIE-Bench_v1'):
    !cp -r /content/drive/MyDrive/PIE-Bench_v1 /content/
    print("Dataset copied from Google Drive!")
elif not os.path.exists(pie_bench_path):
    print("PIE-Bench dataset not found.")
else:
    print(f"PIE-Bench dataset found at {pie_bench_path}")

# Copying mapping file
import shutil
if os.path.exists(f"{pie_bench_path}/mapping_file.json"):
    shutil.copy(f"{pie_bench_path}/mapping_file.json", "/content/data/mapping_file.json")
    print("Mapping file copied!")

"""## 3. Import Libraries and Define Core Functions

"""

import math
import random
import json
import numpy as np
import torch
import torch.nn as nn
from PIL import Image, ImageOps
from torch import autocast
from einops import rearrange
import einops
import k_diffusion as K
from omegaconf import OmegaConf
from tqdm import tqdm
from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt

# Importing from PnPInversion
sys.path.append('/content/PnPInversion/models/instructpix2pix/stable_diffusion')
from ldm.util import instantiate_from_config

print("Imports complete!")

# For Improved v4: Enhanced text encoder (OpenCLIP) and v5
!pip install -q open_clip_torch

import open_clip

device = "cuda" if torch.cuda.is_available() else "cpu"

# Loading a stronger CLIP text encoder
clip_model_name = "hf-hub:laion/CLIP-ViT-H-14-laion2B-s32B-b79K"
enh_clip_model, _, _ = open_clip.create_model_and_transforms(
    clip_model_name, device=device)
enh_tokenizer = open_clip.get_tokenizer(clip_model_name)

enh_clip_model.eval()
for p in enh_clip_model.parameters():
    p.requires_grad = False

# Loading the complete implementation
exec(open('/content/PnPInversion/InstructPix2Pix.py').read())

print("Core functions loaded!")

# Defining CFGDenoiser class
class CFGDenoiser(nn.Module):
    """Classifier-Free Guidance Denoiser for InstructPix2Pix"""
    def __init__(self, model):
        super().__init__()
        self.inner_model = model

    def forward(self, z, sigma, cond, uncond, text_cfg_scale, image_cfg_scale):
        cfg_z = einops.repeat(z, "1 ... -> n ...", n=3)
        cfg_sigma = einops.repeat(sigma, "1 ... -> n ...", n=3)
        cfg_cond = {
            "c_crossattn": [torch.cat([cond["c_crossattn"][0], uncond["c_crossattn"][0], uncond["c_crossattn"][0]])],
            "c_concat": [torch.cat([cond["c_concat"][0], cond["c_concat"][0], uncond["c_concat"][0]])],}
        out_cond, out_img_cond, out_uncond = self.inner_model(cfg_z, cfg_sigma, cond=cfg_cond).chunk(3)
        return out_uncond + text_cfg_scale * (out_cond - out_img_cond) + image_cfg_scale * (out_img_cond - out_uncond)

print("CFGDenoiser class defined!")

!pip install -q taming-transformers
!pip install -q pytorch-lightning
!pip install -q git+https://github.com/CompVis/taming-transformers.git@master

import sys
import os

# Cleaning up any previous taming installs
!pip uninstall -y taming-transformers taming-transformers-rom1504 >/dev/null 2>&1

# Installing the rom1504 fork
!pip install -q taming-transformers-rom1504

# Trying to import VectorQuantizer2, with a fallback alias
try:
    from taming.modules.vqvae.quantize import VectorQuantizer2
    print("VectorQuantizer2 imported successfully!")
except ImportError:
    try:
        from taming.modules.vqvae.quantize import VectorQuantizer
        import taming.modules.vqvae.quantize as q
        q.VectorQuantizer2 = VectorQuantizer
        print("VectorQuantizer2 not found, created alias from VectorQuantizer")
    except Exception as e:
        print("Still failed to import VectorQuantizer2")
        print("Error:", e)

# Loading baseline model
config_path = "/content/PnPInversion/models/instructpix2pix/configs/generate.yaml"
checkpoint_path = "/content/checkpoints/instruct-pix2pix-00-22000.ckpt"

def load_model_from_config(config, ckpt, vae_ckpt=None, verbose=False):
    """Load InstructPix2Pix model from checkpoint"""
    print(f"Loading model from {ckpt}")
    pl_sd = torch.load(ckpt, map_location="cpu")
    if "global_step" in pl_sd:
        print(f"Global Step: {pl_sd['global_step']}")
    sd = pl_sd["state_dict"]
    if vae_ckpt is not None:
        print(f"Loading VAE from {vae_ckpt}")
        vae_sd = torch.load(vae_ckpt, map_location="cpu")["state_dict"]
        sd = {
            k: vae_sd[k[len("first_stage_model."):]]
            if k.startswith("first_stage_model.") else v for k, v in sd.items()}
    model = instantiate_from_config(config.model)
    m, u = model.load_state_dict(sd, strict=False)
    if len(m) > 0 and verbose:
        print("missing keys:", m)
    if len(u) > 0 and verbose:
        print("unexpected keys:", u)
    return model

# config = OmegaConf.load(config_path)
# baseline_model = load_model_from_config(config, checkpoint_path, None)
# baseline_model.eval().cuda()

print("Loading InstructPix2Pix model...")
config = OmegaConf.load(config_path)
baseline_model = load_model_from_config(config, checkpoint_path, None)

# Moving to GPU in half precision to save VRAM
baseline_model = baseline_model.to("cuda")
baseline_model = baseline_model.half()
baseline_model.eval()
device = "cuda"

torch.cuda.empty_cache()

total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
alloc = torch.cuda.memory_allocated() / 1024**3
print(f"Model loaded in fp16. GPU allocated: {alloc:.2f} / {total_mem:.2f} GB")

# Defining editing functions
def mask_decode(encoded_mask, image_shape=[512, 512]):
    """Decode RLE mask to numpy array"""
    length = image_shape[0] * image_shape[1]
    mask_array = np.zeros((length,))
    for i in range(0, len(encoded_mask), 2):
        splice_len = min(encoded_mask[i+1], length - encoded_mask[i])
        for j in range(splice_len):
            mask_array[encoded_mask[i] + j] = 1
    mask_array = mask_array.reshape(image_shape[0], image_shape[1])
    mask_array[0, :] = 1
    mask_array[-1, :] = 1
    mask_array[:, 0] = 1
    mask_array[:, -1] = 1
    return mask_array

def edit_instruct_pix2pix_baseline(
    model, input_path, edit_instruction, resolution=512, steps=50, cfg_text=7.5, cfg_image=1.5, seed=42):
    """Baseline InstructPix2Pix editing function"""
    model_wrap = K.external.CompVisDenoiser(model)
    model_wrap_cfg = CFGDenoiser(model_wrap)
    null_token = model.get_learned_conditioning([""])
    input_image_numpy = Image.open(input_path).convert("RGB")
    width, height = input_image_numpy.size
    factor = resolution / max(width, height)
    factor = math.ceil(min(width, height) * factor / 64) * 64 / min(width, height)
    width = int((width * factor) // 64) * 64
    height = int((height * factor) // 64) * 64
    input_image = ImageOps.fit(input_image_numpy, (width, height), method=Image.Resampling.LANCZOS)
    with torch.no_grad(), autocast("cuda"), model.ema_scope():
        cond = {}
        cond["c_crossattn"] = [model.get_learned_conditioning([edit_instruction])]
        input_image_tensor = 2 * torch.tensor(np.array(input_image)).float() / 255 - 1
        input_image_tensor = rearrange(input_image_tensor, "h w c -> 1 c h w").to(model.device)
        cond["c_concat"] = [model.encode_first_stage(input_image_tensor).mode()]
        uncond = {}
        uncond["c_crossattn"] = [null_token]
        uncond["c_concat"] = [torch.zeros_like(cond["c_concat"][0])]
        sigmas = model_wrap.get_sigmas(steps)
        extra_args = {"cond": cond, "uncond": uncond, "text_cfg_scale": cfg_text, "image_cfg_scale": cfg_image}
        torch.manual_seed(seed)
        z = torch.randn_like(cond["c_concat"][0]) * sigmas[0]
        z = K.sampling.sample_euler_ancestral(model_wrap_cfg, z, sigmas, extra_args=extra_args)
        x = model.decode_first_stage(z)
        x = torch.clamp((x + 1.0) / 2.0, min=0.0, max=1.0)
        x = 255.0 * rearrange(x, "1 c h w -> h w c")
        edited_image = Image.fromarray(x.type(torch.uint8).cpu().numpy())
    return edited_image

print("Editing functions defined!")

# For Improved v4 - EnhancedTextEncoder helper
class EnhancedTextEncoder(nn.Module):
    """
    Wraps a stronger external text encoder (OpenCLIP ViT-H/14).
    Produces a [1, 77, 768]-shaped tensor so it can be fused with
    the SD1.5 CLIP tokens used by InstructPix2Pix.
    """
    def __init__(self, base_token_dim=768, max_tokens=77):
        super().__init__()
        self.base_token_dim = base_token_dim
        self.max_tokens = max_tokens

        # Text embed dim from OpenCLIP (ViT-H/14 uses 1024-d text features)
        self.ext_dim = enh_clip_model.text_projection.shape[1]

        # Learnable projection from OpenCLIP space -> SD CLIP token dim
        self.proj = nn.Linear(self.ext_dim, base_token_dim)

    @torch.no_grad()
    def forward(self, text: str):
        # Tokenizing for OpenCLIP
        tokens = enh_tokenizer([text]).to(device)

        ext_feat = enh_clip_model.encode_text(tokens)
        ext_feat = ext_feat / ext_feat.norm(dim=-1, keepdim=True)
        proj_feat = self.proj(ext_feat)

        # Broadcasting to [1, max_tokens, base_token_dim]
        proj_seq = proj_feat.unsqueeze(1).repeat(1, self.max_tokens, 1)
        return proj_seq

enhanced_text_encoder = EnhancedTextEncoder().to(device)

# Improved v5 to fix version 4
class EnhancedTextEncoder(nn.Module):
    def __init__(self, base_token_dim=768, max_tokens=77):
        super().__init__()
        self.base_token_dim = base_token_dim
        self.max_tokens = max_tokens

        # Inferring external dim once
        with torch.no_grad():
            dummy_tokens = enh_tokenizer(["dummy text"]).to(device)
            dummy_feat = enh_clip_model.encode_text(dummy_tokens)
        ext_dim = dummy_feat.shape[-1]

        self.proj = nn.Linear(ext_dim, base_token_dim)

    def forward(self, text: str) -> torch.Tensor:
        tokens = enh_tokenizer([text]).to(device)
        feat = enh_clip_model.encode_text(tokens)
        feat = feat / feat.norm(dim=-1, keepdim=True)

        proj_feat = self.proj(feat)
        proj_seq  = proj_feat.unsqueeze(1).repeat(1, self.max_tokens, 1)
        return proj_seq

enhanced_text_encoder = EnhancedTextEncoder().to(device)

# For Improved v4 - Fusion helper
def get_enhanced_conditioning(model, edit_instruction: str,
    editing_type_id: str, alpha_by_type=None,):
    """
    Returns a fused text conditioning tensor [1, 77, 768]
    combining:
        - SD1.5 CLIP tokens from model.get_learned_conditioning
        - External OpenCLIP-based tokens from EnhancedTextEncoder
    """
    if alpha_by_type is None:
        alpha_by_type = {"0": 0.4, "1": 0.5, "2": 0.5, "3": 0.3,
            "4": 0.4, "5": 0.5, "6": 0.4, "7": 0.4, "8": 0.3, "9": 0.5,}

    tid = str(editing_type_id)
    alpha = alpha_by_type.get(tid, 0.4)

    # Original SD1.5 CLIP tokens: [1, 77, 768]
    base_tokens = model.get_learned_conditioning([edit_instruction])

    # Enhanced tokens from OpenCLIP: [1, 77, 768]
    enh_tokens = enhanced_text_encoder(edit_instruction)

    # Fuse: convex combination of the two token sequences
    fused_tokens = (1.0 - alpha) * base_tokens + alpha * enh_tokens

    return fused_tokens

# For Improved v5 to fix version 4
def get_enhanced_conditioning(model, edit_instruction: str,
    editing_type_id: str, alpha_by_type=None,) -> torch.Tensor:
    if alpha_by_type is None:
        alpha_by_type = {"0": 0.05, "1": 0.05, "2": 0.05, "3": 0.03, "4": 0.05,
            "5": 0.05, "6": 0.05, "7": 0.05, "8": 0.03, "9": 0.05,}

    tid = str(editing_type_id)
    alpha = alpha_by_type.get(tid, 0.05)

    base_tokens = model.get_learned_conditioning([edit_instruction])
    enh_tokens = enhanced_text_encoder(edit_instruction)

    fused_tokens = (1.0 - alpha) * base_tokens + alpha * enh_tokens
    return fused_tokens

"""## 4. Improved InstructPix2Pix with Adaptive CFG

"""

# Defining Adaptive CFG Scheduler
# Initial improved editing function, have defined 5 advanced ones after this
class AdaptiveCFGScheduler:
    """Adaptive CFG scheduler based on editing type"""
    EDITING_TYPE_CONFIGS = {
        "0": {"cfg_text": 7.5, "cfg_image": 1.5, "steps": 50}, # random
        "1": {"cfg_text": 8.0, "cfg_image": 1.3, "steps": 60}, # change_object
        "2": {"cfg_text": 8.5, "cfg_image": 1.2, "steps": 65}, # add_object
        "3": {"cfg_text": 7.0, "cfg_image": 1.8, "steps": 55}, # delete_object
        "4": {"cfg_text": 7.5, "cfg_image": 1.5, "steps": 50}, # change_attribute_content
        "5": {"cfg_text": 8.0, "cfg_image": 1.4, "steps": 60}, # change_attribute_pose
        "6": {"cfg_text": 7.0, "cfg_image": 1.6, "steps": 50}, # change_attribute_color
        "7": {"cfg_text": 7.5, "cfg_image": 1.5, "steps": 50}, # change_attribute_material
        "8": {"cfg_text": 6.5, "cfg_image": 2.0, "steps": 45}, # change_background
        "9": {"cfg_text": 8.5, "cfg_image": 1.3, "steps": 65}, # change_style
    }
    @classmethod
    def get_config(cls, editing_type_id, base_cfg_text=7.5, base_cfg_image=1.5, base_steps=50):
        if editing_type_id in cls.EDITING_TYPE_CONFIGS:
            return cls.EDITING_TYPE_CONFIGS[editing_type_id]
        return {"cfg_text": base_cfg_text, "cfg_image": base_cfg_image, "steps": base_steps}

def edit_instruct_pix2pix_improved(
    model, input_path, edit_instruction, editing_type_id, resolution=512, seed=42):
    """Improved InstructPix2Pix with adaptive CFG"""
    config = AdaptiveCFGScheduler.get_config(editing_type_id)
    cfg_text, cfg_image, steps = config["cfg_text"], config["cfg_image"], config["steps"]
    model_wrap = K.external.CompVisDenoiser(model)
    model_wrap_cfg = CFGDenoiser(model_wrap)
    null_token = model.get_learned_conditioning([""])
    input_image_numpy = Image.open(input_path).convert("RGB")
    width, height = input_image_numpy.size
    factor = resolution / max(width, height)
    factor = math.ceil(min(width, height) * factor / 64) * 64 / min(width, height)
    width = int((width * factor) // 64) * 64
    height = int((height * factor) // 64) * 64
    input_image = ImageOps.fit(input_image_numpy, (width, height), method=Image.Resampling.LANCZOS)
    with torch.no_grad(), autocast("cuda"), model.ema_scope():
        cond = {}
        cond["c_crossattn"] = [model.get_learned_conditioning([edit_instruction])]
        input_image_tensor = 2 * torch.tensor(np.array(input_image)).float() / 255 - 1
        input_image_tensor = rearrange(input_image_tensor, "h w c -> 1 c h w").to(model.device)
        cond["c_concat"] = [model.encode_first_stage(input_image_tensor).mode()]
        uncond = {}
        uncond["c_crossattn"] = [null_token]
        uncond["c_concat"] = [torch.zeros_like(cond["c_concat"][0])]
        sigmas = model_wrap.get_sigmas(steps)
        extra_args = {"cond": cond, "uncond": uncond, "text_cfg_scale": cfg_text, "image_cfg_scale": cfg_image}
        torch.manual_seed(seed)
        z = torch.randn_like(cond["c_concat"][0]) * sigmas[0]
        z = K.sampling.sample_euler_ancestral(model_wrap_cfg, z, sigmas, extra_args=extra_args)
        x = model.decode_first_stage(z)
        x = torch.clamp((x + 1.0) / 2.0, min=0.0, max=1.0)
        x = 255.0 * rearrange(x, "1 c h w -> h w c")
        edited_image = Image.fromarray(x.type(torch.uint8).cpu().numpy())
    return edited_image

print("Improved editing function defined!")

"""## 5. Metrics Calculator

"""

# Initializing metrics calculator
from torchmetrics.image import StructuralSimilarityIndexMeasure
from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
from torchmetrics.multimodal import CLIPScore

class MetricsCalculator:
    def __init__(self, device="cuda"):
        self.device = device
        self.clip_metric = CLIPScore(model_name_or_path="openai/clip-vit-large-patch14").to(device)
        self.lpips_metric = LearnedPerceptualImagePatchSimilarity(net_type='alex').to(device)
        self.ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)

    def calculate_clip_similarity(self, img, txt, mask=None):
        img_array = np.array(img)
        if mask is not None:
            mask_array = np.array(mask)
            if len(mask_array.shape) == 2:
                mask_array = mask_array[:, :, np.newaxis]
            img_array = np.uint8(img_array * mask_array)
        img_tensor = torch.tensor(img_array).permute(2, 0, 1).unsqueeze(0).to(self.device)
        score = self.clip_metric(img_tensor, [txt])
        return score.cpu().item()

    def calculate_lpips(self, img_pred, img_gt, mask_pred=None, mask_gt=None):
        img_pred = np.array(img_pred).astype(np.float32) / 255
        img_gt = np.array(img_gt).astype(np.float32) / 255
        if mask_pred is not None:
            mask_pred = np.array(mask_pred).astype(np.float32)
            if len(mask_pred.shape) == 2:
                mask_pred = mask_pred[:, :, np.newaxis]
            img_pred = img_pred * mask_pred
        if mask_gt is not None:
            mask_gt = np.array(mask_gt).astype(np.float32)
            if len(mask_gt.shape) == 2:
                mask_gt = mask_gt[:, :, np.newaxis]
            img_gt = img_gt * mask_gt
        img_pred_tensor = torch.tensor(img_pred).permute(2, 0, 1).unsqueeze(0).to(self.device)
        img_gt_tensor = torch.tensor(img_gt).permute(2, 0, 1).unsqueeze(0).to(self.device)
        score = self.lpips_metric(img_pred_tensor * 2 - 1, img_gt_tensor * 2 - 1)
        return score.cpu().item()

    def calculate_ssim(self, img_pred, img_gt, mask_pred=None, mask_gt=None):
        img_pred = np.array(img_pred).astype(np.float32) / 255
        img_gt = np.array(img_gt).astype(np.float32) / 255
        if mask_pred is not None:
            mask_pred = np.array(mask_pred).astype(np.float32)
            if len(mask_pred.shape) == 2:
                mask_pred = mask_pred[:, :, np.newaxis]
            img_pred = img_pred * mask_pred
        if mask_gt is not None:
            mask_gt = np.array(mask_gt).astype(np.float32)
            if len(mask_gt.shape) == 2:
                mask_gt = mask_gt[:, :, np.newaxis]
            img_gt = img_gt * mask_gt
        img_pred_tensor = torch.tensor(img_pred).permute(2, 0, 1).unsqueeze(0).to(self.device)
        img_gt_tensor = torch.tensor(img_gt).permute(2, 0, 1).unsqueeze(0).to(self.device)
        score = self.ssim_metric(img_pred_tensor, img_gt_tensor)
        return score.cpu().item()

metrics_calc = MetricsCalculator(device="cuda")
print("Metrics calculator initialized!")

"""## 6. Load PIE-Bench Dataset

"""

# Loading mapping file
mapping_file_path = "/content/data/mapping_file.json"

with open(mapping_file_path, 'r') as f:
    editing_instructions = json.load(f)

print(f"Loaded {len(editing_instructions)} editing instructions")
print(f"Sample keys: {list(editing_instructions.keys())[:5]}")

# Editing type names
EDITING_TYPE_NAMES = {
    "0": "random", "1": "change_object", "2": "add_object", "3": "delete_object",
    "4": "change_attribute_content", "5": "change_attribute_pose",
    "6": "change_attribute_color", "7": "change_attribute_material",
    "8": "change_background", "9": "change_style"}

"""## 7. Run Evaluation (Baseline)

"""

print("GPU allocated:", torch.cuda.memory_allocated()/1024**3, "GB")

# Evaluating baseline InstructPix2Pix
baseline_output_dir = "/content/output/baseline_instructpix2pix"
os.makedirs(baseline_output_dir, exist_ok=True)

baseline_results = []
pie_bench_path = "/content/PIE-Bench_v1"

print("Starting baseline evaluation...")
for idx, (key, item) in enumerate(tqdm(editing_instructions.items(), desc="Baseline")):
    try:
        image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        editing_instruction = item["editing_instruction"]
        output_path = os.path.join(baseline_output_dir, item["image_path"])
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        if os.path.exists(output_path):
            continue

        edited_image = edit_instruct_pix2pix_baseline(
            baseline_model, image_path, editing_instruction,
            resolution=512, steps=50, cfg_text=7.5, cfg_image=1.5, seed=42)
        edited_image.save(output_path)

        baseline_results.append({
            "key": key,
            "image_path": item["image_path"],
            "editing_type_id": item["editing_type_id"],
            "output_path": output_path})

        if (idx + 1) % 50 == 0:
            print(f"Processed {idx + 1} images...")
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error processing {key}: {str(e)}")
        continue

print(f"Baseline evaluation complete! Processed {len(baseline_results)} images.")

print("Editing types in baseline_results:", sorted({item["editing_type_id"] for item in baseline_results}))

"""## 8. Run Evaluation (Improved)

"""

# Improved InstructPix2Pix: Type-aware CFG + Source-Edit Blending
class AdaptiveEditConfig:
    """
    Per-edit-type configuration:
      - text guidance scale (cfg_text)
      - image guidance scale (cfg_image)
      - number of diffusion steps (steps)
      - blend_alpha: how much of the edited result vs original image to keep
    """
    EDITING_TYPE_CONFIGS = {
        # Baseline was: cfg_text=7.5, cfg_image=1.5, steps=50, and no blending.
        "0": {"cfg_text": 8.5, "cfg_image": 1.3, "steps": 50, "blend_alpha": 0.70}, # random
        "1": {"cfg_text": 9.0, "cfg_image": 1.0, "steps": 60, "blend_alpha": 0.75}, # change_object
        "2": {"cfg_text": 9.0, "cfg_image": 1.1, "steps": 60, "blend_alpha": 0.80}, # add_object
        "3": {"cfg_text": 7.5, "cfg_image": 1.8, "steps": 55, "blend_alpha": 0.60}, # delete_object
        "4": {"cfg_text": 8.0, "cfg_image": 1.4, "steps": 45, "blend_alpha": 0.50}, # change_attribute_content
        "5": {"cfg_text": 8.5, "cfg_image": 1.3, "steps": 55, "blend_alpha": 0.60}, # change_attribute_pose
        "6": {"cfg_text": 7.0, "cfg_image": 2.0, "steps": 40, "blend_alpha": 0.45}, # change_attribute_color
        "7": {"cfg_text": 8.0, "cfg_image": 1.6, "steps": 45, "blend_alpha": 0.50}, # change_attribute_material
        "8": {"cfg_text": 7.0, "cfg_image": 2.2, "steps": 50, "blend_alpha": 0.70}, # change_background
        "9": {"cfg_text": 9.5, "cfg_image": 1.2, "steps": 60, "blend_alpha": 0.70}, # change_style
    }

    @classmethod
    def get_config(
        cls, editing_type_id, base_cfg_text=7.5, base_cfg_image=1.5, base_steps=50, base_blend=0.7,):
        tid = str(editing_type_id)
        cfg = cls.EDITING_TYPE_CONFIGS.get(
            tid,{
                "cfg_text": base_cfg_text,
                "cfg_image": base_cfg_image,
                "steps": base_steps,
                "blend_alpha": base_blend,
            },)
        return cfg

def edit_instruct_pix2pix_improved(
    model, input_path, edit_instruction, editing_type_id, resolution=512, seed=42,):
    """
    Improved InstructPix2Pix:

    - Per-type cfg_text / cfg_image / steps (AdaptiveEditConfig).
    - Type-dependent blending between the original image and edited output
      to preserve structure and appearance where appropriate.
    - No model.ema_scope() (avoids an extra EMA copy -> lower VRAM).
    - Compatible with fp16 baseline_model (model.half()).
    """

    # Getting per-type hyperparameters
    config = AdaptiveEditConfig.get_config(editing_type_id)
    cfg_text = config["cfg_text"]
    cfg_image = config["cfg_image"]
    steps = config["steps"]
    blend_alpha = config["blend_alpha"]
    # how strong the edit is in pixel space

    # Wrapping model with K-diffusion denoisers
    model_wrap = K.external.CompVisDenoiser(model)
    model_wrap_cfg = CFGDenoiser(model_wrap)

    # Conditioning and image preprocessing
    null_token = model.get_learned_conditioning([""])
    input_image_numpy = Image.open(input_path).convert("RGB")

    # Resizing to multiple of 64, preserving aspect ratio (same logic as baseline)
    width, height = input_image_numpy.size
    factor = resolution / max(width, height)
    factor = math.ceil(min(width, height) * factor / 64) * 64 / min(width, height)
    width = int((width * factor) // 64) * 64
    height = int((height * factor) // 64) * 64
    input_image = ImageOps.fit(
        input_image_numpy, (width, height), method=Image.Resampling.LANCZOS)

    with torch.no_grad(), autocast("cuda"):
        cond = {}
        cond["c_crossattn"] = [model.get_learned_conditioning([edit_instruction])]

        # Image -> tensor in [-1, 1], CHW, batch, on model device
        input_image_tensor = 2 * torch.tensor(np.array(input_image)).float() / 255 - 1
        input_image_tensor = rearrange(input_image_tensor, "h w c -> 1 c h w").to(
            model.device)
        input_image_tensor = input_image_tensor.half()

        # Keeping a [0, 1] version for blending later
        input_image_01 = torch.clamp(
            (input_image_tensor + 1.0) / 2.0, min=0.0, max=1.0)

        # Encoding image to latent space
        encoded = model.encode_first_stage(input_image_tensor).mode()
        cond["c_concat"] = [encoded]

        uncond = {}
        uncond["c_crossattn"] = [null_token]
        uncond["c_concat"] = [torch.zeros_like(encoded)]

        # Sampling with per-type steps and CFG scales
        sigmas = model_wrap.get_sigmas(steps).to(model.device)
        extra_args = {
            "cond": cond,
            "uncond": uncond,
            "text_cfg_scale": cfg_text,
            "image_cfg_scale": cfg_image,}

        torch.manual_seed(seed)
        z = torch.randn_like(encoded) * sigmas[0]
        z = K.sampling.sample_euler_ancestral(
            model_wrap_cfg, z, sigmas, extra_args=extra_args)

        # Decoding edited latent to image in [-1, 1]
        x_edit = model.decode_first_stage(z)

        # Converting edited to [0, 1]
        x_edit_01 = torch.clamp((x_edit + 1.0) / 2.0, min=0.0, max=1.0)

        # Type-dependent blending: preserve structure / identity more for some types
        # x_final_01 = (1 - blend_alpha) * original + blend_alpha * edited
        x_final_01 = (1.0 - blend_alpha) * input_image_01 + blend_alpha * x_edit_01

        x = 255.0 * rearrange(x_final_01, "1 c h w -> h w c")
        edited_image = Image.fromarray(x.type(torch.uint8).cpu().numpy())

    return edited_image

print("New improved editing function defined!")

# Evaluating improved InstructPix2Pix
improved_output_dir = "/content/output/improved_instructpix2pix"
os.makedirs(improved_output_dir, exist_ok=True)

improved_results = []

print("Starting improved evaluation...")
for idx, (key, item) in enumerate(tqdm(editing_instructions.items(), desc="Improved")):
    try:
        image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        editing_instruction = item["editing_instruction"]
        editing_type_id = item["editing_type_id"]
        output_path = os.path.join(improved_output_dir, item["image_path"])
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        edited_image = edit_instruct_pix2pix_improved(
            baseline_model, image_path, editing_instruction, editing_type_id,
            resolution=512, seed=42)
        edited_image.save(output_path)

        improved_results.append({
            "key": key,
            "image_path": item["image_path"],
            "editing_type_id": editing_type_id,
            "output_path": output_path})

        if (idx + 1) % 50 == 0:
            print(f"Processed {idx + 1} images...")
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error processing {key}: {str(e)}")
        continue

print(f"Improved evaluation complete! Processed {len(improved_results)} images.")

# Improved v3 InstructPix2Pix: CLIP-oriented 2-stage CFG + Blending
class AdaptiveEditConfigV3:
    """
    Per-edit-type configuration for Improved v3 (CLIP-focused):
      - steps: total diffusion steps
      - split_ratio: fraction of steps for stage 1 (structure-preserving)
      - cfg1_text, cfg1_image: guidance scales for stage 1
      - cfg2_text, cfg2_image: guidance scales for stage 2 (semantic stage)
      - blend_alpha: final pixel-space mix between original and edited
      x_final = (1 - blend_alpha) * x_orig + blend_alpha * x_edit
    """

    EDITING_TYPE_CONFIGS = {
        # Heuristic design:
        #  - For types where v2 CLIP got worse (0,1,2,6,7,8,9) →
        # longer stage 2, higher text CFG, slightly lower image CFG, stronger blending.
        #  - For types where v2 CLIP was much better (3,4,5) → keeping a v2-like setup.

        # CLIP dropped in v2 → push text more
        "0": {
            "steps": 50,
            "split_ratio": 0.45,
            "cfg1_text": 7.0, "cfg1_image": 1.8,
            "cfg2_text": 10.0, "cfg2_image": 1.1,
            "blend_alpha": 0.80,
        },

        "1": {
            "steps": 55,
            "split_ratio": 0.45,
            "cfg1_text": 7.0, "cfg1_image": 2.0,
            "cfg2_text": 10.0, "cfg2_image": 1.1,
            "blend_alpha": 0.82,
        },

        "2": {
            "steps": 55,
            "split_ratio": 0.45,
            "cfg1_text": 7.2, "cfg1_image": 1.9,
            "cfg2_text": 10.2, "cfg2_image": 1.1,
            "blend_alpha": 0.85,
        },

        "3": {
            "steps": 55,
            "split_ratio": 0.5,
            "cfg1_text": 7.0, "cfg1_image": 1.7,
            "cfg2_text": 9.2, "cfg2_image": 1.2,
            "blend_alpha": 0.60,
        },

        "4": {
            "steps": 45,
            "split_ratio": 0.55,
            "cfg1_text": 7.5, "cfg1_image": 1.7,
            "cfg2_text": 9.0, "cfg2_image": 1.3,
            "blend_alpha": 0.60,
        },

        "5": {
            "steps": 50,
            "split_ratio": 0.55,
            "cfg1_text": 7.8, "cfg1_image": 1.6,
            "cfg2_text": 9.0, "cfg2_image": 1.3,
            "blend_alpha": 0.62,
        },

        "6": {
            "steps": 45,
            "split_ratio": 0.45,
            "cfg1_text": 7.2, "cfg1_image": 2.0,
            "cfg2_text": 10.0, "cfg2_image": 1.2,
            "blend_alpha": 0.82,
        },

        "7": {
            "steps": 45,
            "split_ratio": 0.45,
            "cfg1_text": 7.5, "cfg1_image": 1.9,
            "cfg2_text": 10.0, "cfg2_image": 1.2,
            "blend_alpha": 0.80,
        },

        "8": {
            "steps": 50,
            "split_ratio": 0.45,
            "cfg1_text": 7.0, "cfg1_image": 2.0,
            "cfg2_text": 9.8, "cfg2_image": 1.3,
            "blend_alpha": 0.80,
        },

        # CLIP dropped the most in v2 → very text-heavy
        "9": {
            "steps": 55,
            "split_ratio": 0.45,
            "cfg1_text": 7.0, "cfg1_image": 1.9,
            "cfg2_text": 10.5, "cfg2_image": 1.1,
            "blend_alpha": 0.85,
        },
    }

    @classmethod
    def get_config(
        cls, editing_type_id, base_steps=50, base_split_ratio=0.5, base_cfg1_text=7.5,
        base_cfg1_image=1.5, base_cfg2_text=9.0, base_cfg2_image=1.3, base_blend=0.75,):
        tid = str(editing_type_id)
        return cls.EDITING_TYPE_CONFIGS.get(
            tid,{
                "steps": base_steps,
                "split_ratio": base_split_ratio,
                "cfg1_text": base_cfg1_text,
                "cfg1_image": base_cfg1_image,
                "cfg2_text": base_cfg2_text,
                "cfg2_image": base_cfg2_image,
                "blend_alpha": base_blend,
            },)

def edit_instruct_pix2pix_improved3(
    model, input_path, edit_instruction, editing_type_id, resolution=512, seed=42,):
    """
    Improved v3 InstructPix2Pix (CLIP-oriented):
    - Two-stage K-diffusion sampling:
        Stage 1: strong image CFG → preserve structure.
        Stage 2: strong text CFG → enforce semantic edit and boost CLIP.
    - Per-type schedule and blending parameters (see AdaptiveEditConfigV3).
    - Pixel-space blending between original and edited image.
    """

    cfg = AdaptiveEditConfigV3.get_config(editing_type_id)
    steps = cfg["steps"]
    split_ratio = cfg["split_ratio"]
    cfg1_text = cfg["cfg1_text"]
    cfg1_image = cfg["cfg1_image"]
    cfg2_text = cfg["cfg2_text"]
    cfg2_image = cfg["cfg2_image"]
    blend_alpha = cfg["blend_alpha"]

    # Wrapping model with K-diffusion denoisers
    model_wrap = K.external.CompVisDenoiser(model)
    model_wrap_cfg = CFGDenoiser(model_wrap)

    # Loading and resizing input image
    null_token = model.get_learned_conditioning([""])
    input_image_np = Image.open(input_path).convert("RGB")

    width, height = input_image_np.size
    factor = resolution / max(width, height)
    factor = math.ceil(min(width, height) * factor / 64) * 64 / min(width, height)
    width = int((width * factor) // 64) * 64
    height = int((height * factor) // 64) * 64
    input_image = ImageOps.fit(
        input_image_np, (width, height), method=Image.Resampling.LANCZOS)

    device = model.device

    with torch.no_grad(), autocast("cuda"):
        # Building conditioning
        cond = {}
        cond["c_crossattn"] = [model.get_learned_conditioning([edit_instruction])]

        img_tensor = 2 * torch.tensor(np.array(input_image)).float() / 255 - 1
        img_tensor = rearrange(img_tensor, "h w c -> 1 c h w").to(device)
        img_tensor = img_tensor.half()

        img_01 = torch.clamp((img_tensor + 1.0) / 2.0, 0.0, 1.0)

        encoded = model.encode_first_stage(img_tensor).mode()
        cond["c_concat"] = [encoded]

        uncond = {
            "c_crossattn": [null_token],
            "c_concat":    [torch.zeros_like(encoded)],}

        # Sigma scheduling and splitting
        sigmas_full = model_wrap.get_sigmas(steps).to(device)
        split_idx = int(split_ratio * (steps - 1))
        split_idx = max(1, min(steps - 2, split_idx))

        sigmas1 = sigmas_full[: split_idx + 1]
        sigmas2 = sigmas_full[split_idx:]

        extra_args1 = {
            "cond": cond,
            "uncond": uncond,
            "text_cfg_scale": cfg1_text,
            "image_cfg_scale": cfg1_image,}
        extra_args2 = {
            "cond": cond,
            "uncond": uncond,
            "text_cfg_scale": cfg2_text,
            "image_cfg_scale": cfg2_image,}

        # Two-stage sampling
        torch.manual_seed(seed)
        z = torch.randn_like(encoded) * sigmas1[0]
        z = K.sampling.sample_euler_ancestral(model_wrap_cfg, z, sigmas1, extra_args=extra_args1)
        z = K.sampling.sample_euler_ancestral(model_wrap_cfg, z, sigmas2, extra_args=extra_args2)

        # Decoding and blending
        x_edit = model.decode_first_stage(z)
        x_edit_01 = torch.clamp((x_edit + 1.0) / 2.0, 0.0, 1.0)

        x_final_01 = (1.0 - blend_alpha) * img_01 + blend_alpha * x_edit_01

        x = 255.0 * rearrange(x_final_01, "1 c h w -> h w c")
        edited_image = Image.fromarray(x.type(torch.uint8).cpu().numpy())

    return edited_image

print("Improved v3 editing function defined!")

improved3_output_dir = "/content/output/improved_instructpix2pix_v3"
os.makedirs(improved3_output_dir, exist_ok=True)

improved3_results = []

print("Starting improved v3 evaluation...")
for idx, (key, item) in enumerate(tqdm(editing_instructions.items(), desc="Improved v3")):
    try:
        image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        editing_instruction = item["editing_instruction"]
        editing_type_id = item["editing_type_id"]

        output_path = os.path.join(improved3_output_dir, item["image_path"])
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        edited_image = edit_instruct_pix2pix_improved3(
            baseline_model, image_path, editing_instruction, editing_type_id,
            resolution=512, seed=42,)
        edited_image.save(output_path)

        improved3_results.append({
            "key": key,
            "image_path": item["image_path"],
            "editing_type_id": editing_type_id,
            "output_path": output_path,})

        if (idx + 1) % 50 == 0:
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error processing {key}: {e}")
        continue

print(f"Improved v3 evaluation complete! Processed {len(improved3_results)} images.")

print("Editing types in baseline_results:", sorted({item["editing_type_id"] for item in baseline_results}))
print("Editing types in improved_results:", sorted({item["editing_type_id"] for item in improved_results}))

# Improved v5 editing function with Enhanced Text Encoder + AdaptiveEditConfigV3
def edit_instruct_pix2pix_improved5(model, input_path: str, edit_instruction: str,
    editing_type_id: str, resolution: int = 512, seed: int = 42,):
    """
    Improved v5 InstructPix2Pix with Enhanced Text Encoder:
    - Uses AdaptiveEditConfigV3 per edit type:
        * steps: total diffusion steps
        * split_ratio: fraction of steps for stage 1 (structure-preserving)
        * cfg1_text, cfg1_image: CFG scales for stage 1
        * cfg2_text, cfg2_image: CFG scales for stage 2 (semantic stage)
        * blend_alpha: pixel-space blend weight
    - Replaces vanilla text conditioning with fused
      (SD1.5 CLIP + OpenCLIP ViT-H/14) tokens via get_enhanced_conditioning.
    """
    # Per-type hyperparameters from AdaptiveEditConfigV3
    cfg = AdaptiveEditConfigV3.get_config(editing_type_id)
    steps = cfg["steps"]
    split_ratio = cfg["split_ratio"]
    cfg1_text = cfg["cfg1_text"]
    cfg1_image = cfg["cfg1_image"]
    cfg2_text = cfg["cfg2_text"]
    cfg2_image = cfg["cfg2_image"]
    blend_alpha = cfg["blend_alpha"]

    # Wrapping model with K-diffusion + CFG denoiser
    model_wrap = K.external.CompVisDenoiser(model)
    model_wrap_cfg = CFGDenoiser(model_wrap)
    null_token = model.get_learned_conditioning([""])

    # Loading and resizing input image
    input_image = Image.open(input_path).convert("RGB")
    width, height = input_image.size

    short_side = min(width, height)
    if short_side <= 0:
        raise ValueError(f"Invalid image size: {width}x{height}")

    # Scaling so that short side ~ resolution, then snap to 64 grid
    factor = resolution / short_side
    factor = math.ceil(short_side * factor / 64) * 64 / short_side
    width = int((width * factor) // 64) * 64
    height = int((height * factor) // 64) * 64

    input_image = ImageOps.fit(
        input_image, (width, height), method=Image.Resampling.LANCZOS)

    # Diffusion sampling with 2-stage schedule
    with torch.no_grad(), autocast("cuda"):
        # text conditioning: enhanced encoder + fusion
        cond = {}
        cond["c_crossattn"] = [
            get_enhanced_conditioning(model,
                edit_instruction=edit_instruction,
                editing_type_id=editing_type_id,)]

        # Image -> tensor in [-1, 1], CHW, batch, fp16 on model.device
        input_image_tensor = 2 * torch.tensor(np.array(input_image)).float() / 255.0 - 1.0
        input_image_tensor = rearrange(input_image_tensor, "h w c -> 1 c h w").to(
            model.device)
        input_image_tensor = input_image_tensor.half()

        # Keeping [0, 1] version for blending later
        input_image_01 = torch.clamp(
            (input_image_tensor + 1.0) / 2.0, min=0.0, max=1.0)

        # Encoding image to latent
        encoded = model.encode_first_stage(input_image_tensor).mode()
        cond["c_concat"] = [encoded]

        # Unconditional branch
        uncond = {
            "c_crossattn": [null_token],
            "c_concat": [torch.zeros_like(encoded)],}

        # 2-stage sigma schedule
        sigmas = model_wrap.get_sigmas(steps).to(model.device)

        # Ensuring at least 1 step in each stage
        split_idx = max(1, min(len(sigmas) - 1, int(len(sigmas) * split_ratio)))
        sigmas1 = sigmas[:split_idx]
        sigmas2 = sigmas[split_idx - 1 :]

        extra_args1 = {
            "cond": cond,
            "uncond": uncond,
            "text_cfg_scale": cfg1_text,
            "image_cfg_scale": cfg1_image,}

        extra_args2 = {
            "cond": cond,
            "uncond": uncond,
            "text_cfg_scale": cfg2_text,
            "image_cfg_scale": cfg2_image,}

        # Stage 1 + Stage 2 sampling
        torch.manual_seed(seed)
        z = torch.randn_like(encoded) * sigmas1[0]

        # Stage 1: structure-preserving (stronger image CFG)
        z = K.sampling.sample_euler_ancestral(
            model_wrap_cfg, z, sigmas1, extra_args=extra_args1)

        # Stage 2: semantic refinement (stronger text CFG)
        z = K.sampling.sample_euler_ancestral(
            model_wrap_cfg, z, sigmas2, extra_args=extra_args2)

        # Decoding latent to image in [-1, 1]
        x_edit = model.decode_first_stage(z)

        # Converting edited to [0, 1]
        x_edit_01 = torch.clamp((x_edit + 1.0) / 2.0, min=0.0, max=1.0)

        # Type-dependent blending in pixel space
        x_final_01 = (1.0 - blend_alpha) * input_image_01 + blend_alpha * x_edit_01

        x = 255.0 * rearrange(x_final_01, "1 c h w -> h w c")
        edited_image = Image.fromarray(x.type(torch.uint8).cpu().numpy())

    return edited_image

print("Enhanced-text Improved v5 editing function defined!")

# Training EnhancedTextEncoder.proj to mimic SD text tokens
import random

# Collecting unique editing instructions from PIE-Bench
all_texts = sorted({item["editing_instruction"] for item in editing_instructions.values()})
print(f"Number of unique editing instructions: {len(all_texts)}")

# Setting EnhancedTextEncoder to train mode
enhanced_text_encoder.train()
for p in enhanced_text_encoder.parameters():
    p.requires_grad = False

# Only training the projection layer
for p in enhanced_text_encoder.proj.parameters():
    p.requires_grad = True

optimizer = torch.optim.Adam(enhanced_text_encoder.proj.parameters(), lr=1e-4)

num_epochs = 25
print_every = max(1, len(all_texts) // 5)
device = baseline_model.device

for epoch in range(num_epochs):
    random.shuffle(all_texts)
    total_loss = 0.0

    for i, text in enumerate(all_texts):
        optimizer.zero_grad()

        # Target tokens: SD1.5 CLIP tokens
        with torch.no_grad():
            base_tokens = baseline_model.get_learned_conditioning([text])
            base_tokens = base_tokens.detach().to(device=device, dtype=torch.float32)

        # Predicted tokens: EnhancedTextEncoder output
        pred_tokens = enhanced_text_encoder(text)

        # MSE loss between sequences
        loss = ((pred_tokens - base_tokens) ** 2).mean()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        if (i + 1) % print_every == 0:
            avg_so_far = total_loss / (i + 1)
            print(f"  Epoch {epoch+1}/{num_epochs}, step {i+1}/{len(all_texts)}, "
                  f"avg MSE so far: {avg_so_far:.6f}")

    avg_loss = total_loss / len(all_texts)
    print(f"[Epoch {epoch+1}/{num_epochs}] Average MSE: {avg_loss:.6f}")

# Switching back to eval mode & freeze
enhanced_text_encoder.eval()
for p in enhanced_text_encoder.parameters():
    p.requires_grad = False

print("Training of EnhancedTextEncoder.proj complete! Ready for v5 evaluation.")

improved5_output_dir = "/content/output/improved_instructpix2pix_v5"
os.makedirs(improved5_output_dir, exist_ok=True)

improved5_results = []

print("Starting improved v5 evaluation...")
for idx, (key, item) in enumerate(tqdm(editing_instructions.items(), desc="Improved v5")):
    try:
        image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        editing_instruction = item["editing_instruction"]
        editing_type_id = item["editing_type_id"]

        output_path = os.path.join(improved5_output_dir, item["image_path"])
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # v5 editing function
        edited_image = edit_instruct_pix2pix_improved5(
            baseline_model, image_path, editing_instruction,
            editing_type_id, resolution=512, seed=42,)
        edited_image.save(output_path)

        improved5_results.append({
            "key": key,
            "image_path": item["image_path"],
            "editing_type_id": editing_type_id,
            "output_path": output_path,})

        if (idx + 1) % 50 == 0:
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error processing {key}: {e}")
        continue

print(f"Improved v5 evaluation complete! Processed {len(improved5_results)} images.")

"""## 9. Compute Metrics

"""

# Computing metrics for baseline
baseline_metrics = []

print("Computing baseline metrics...")
for item in tqdm(baseline_results, desc="Baseline Metrics"):
    try:
        key = item["key"]
        editing_info = editing_instructions[key]
        src_image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        src_image = Image.open(src_image_path).convert("RGB")
        tgt_image_path = item["output_path"]
        tgt_image = Image.open(tgt_image_path).convert("RGB")
        if tgt_image.size[0] != tgt_image.size[1]:
            tgt_image = tgt_image.crop((tgt_image.size[0]-512, tgt_image.size[1]-512,
                                      tgt_image.size[0], tgt_image.size[1]))
        ssim = metrics_calc.calculate_ssim(src_image, tgt_image)
        lpips = metrics_calc.calculate_lpips(src_image, tgt_image)
        clip_sim = metrics_calc.calculate_clip_similarity(
            tgt_image, editing_info["editing_prompt"].replace("[", "").replace("]", ""))
        baseline_metrics.append({
            "key": key, "editing_type_id": item["editing_type_id"],
            "ssim": ssim, "lpips": lpips, "clip_similarity": clip_sim})
    except Exception as e:
        print(f"Error computing metrics for {item['key']}: {str(e)}")
        continue

print(f"Baseline metrics computed for {len(baseline_metrics)} images.")

# Computing metrics for improved method
improved_metrics = []

print("Computing improved metrics...")
for item in tqdm(improved_results, desc="Improved Metrics"):
    try:
        key = item["key"]
        editing_info = editing_instructions[key]
        src_image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        src_image = Image.open(src_image_path).convert("RGB")
        tgt_image_path = item["output_path"]
        tgt_image = Image.open(tgt_image_path).convert("RGB")
        if tgt_image.size[0] != tgt_image.size[1]:
            tgt_image = tgt_image.crop((tgt_image.size[0]-512, tgt_image.size[1]-512,
                                      tgt_image.size[0], tgt_image.size[1]))
        ssim = metrics_calc.calculate_ssim(src_image, tgt_image)
        lpips = metrics_calc.calculate_lpips(src_image, tgt_image)
        clip_sim = metrics_calc.calculate_clip_similarity(
            tgt_image, editing_info["editing_prompt"].replace("[", "").replace("]", ""))
        improved_metrics.append({
            "key": key, "editing_type_id": item["editing_type_id"],
            "ssim": ssim, "lpips": lpips, "clip_similarity": clip_sim})
    except Exception as e:
        print(f"Error computing metrics for {item['key']}: {str(e)}")
        continue

print(f"Improved metrics computed for {len(improved_metrics)} images.")

# Computing metrics for improved v3
improved3_metrics = []

print("Computing Improved v3 metrics...")
for item in tqdm(improved3_results, desc="Improved v3 Metrics"):
    try:
        key = item["key"]
        editing_info = editing_instructions[key]
        src_image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        src_image = Image.open(src_image_path).convert("RGB")
        tgt_image_path = item["output_path"]
        tgt_image = Image.open(tgt_image_path).convert("RGB")
        if tgt_image.size[0] != tgt_image.size[1]:
            tgt_image = tgt_image.crop(
                (tgt_image.size[0] - 512, tgt_image.size[1] - 512,
                 tgt_image.size[0], tgt_image.size[1]))
        ssim = metrics_calc.calculate_ssim(src_image, tgt_image)
        lpips = metrics_calc.calculate_lpips(src_image, tgt_image)
        clip_sim = metrics_calc.calculate_clip_similarity(
            tgt_image, editing_info["editing_prompt"].replace("[", "").replace("]", ""))

        improved3_metrics.append({
            "key": key, "editing_type_id": item["editing_type_id"], "ssim": ssim,
            "lpips": lpips, "clip_similarity": clip_sim,})

    except Exception as e:
        print(f"Error computing metrics for {item['key']}: {str(e)}")
        continue

print(f"Improved v3 metrics computed for {len(improved3_metrics)} images.")

# Computing metrics for improved v5
improved5_metrics = []

print("Computing Improved v5 metrics...")
for item in tqdm(improved5_results, desc="Improved v5 Metrics"):
    try:
        key = item["key"]
        editing_info = editing_instructions[key]
        src_image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        src_image = Image.open(src_image_path).convert("RGB")
        tgt_image_path = item["output_path"]
        tgt_image = Image.open(tgt_image_path).convert("RGB")
        if tgt_image.size[0] != tgt_image.size[1]:
            tgt_image = tgt_image.crop(
                (tgt_image.size[0] - 512, tgt_image.size[1] - 512,
                 tgt_image.size[0], tgt_image.size[1]))
        ssim = metrics_calc.calculate_ssim(src_image, tgt_image)
        lpips = metrics_calc.calculate_lpips(src_image, tgt_image)
        clip_sim = metrics_calc.calculate_clip_similarity(
            tgt_image, editing_info["editing_prompt"].replace("[", "").replace("]", ""))

        improved5_metrics.append({
            "key": key, "editing_type_id": item["editing_type_id"], "ssim": ssim,
            "lpips": lpips, "clip_similarity": clip_sim,})

    except Exception as e:
        print(f"Error computing metrics for {item['key']}: {str(e)}")
        continue

print(f"Improved v5 metrics computed for {len(improved5_metrics)} images.")

"""## 10. Aggregate and Report Results

"""

# Aggregating metrics
def aggregate_metrics(metrics_list, method_name):
    overall = {
        "method": method_name, "num_images": len(metrics_list),
        "ssim_mean": np.mean([m["ssim"] for m in metrics_list]),
        "ssim_std": np.std([m["ssim"] for m in metrics_list]),
        "lpips_mean": np.mean([m["lpips"] for m in metrics_list]),
        "lpips_std": np.std([m["lpips"] for m in metrics_list]),
        "clip_similarity_mean": np.mean([m["clip_similarity"] for m in metrics_list]),
        "clip_similarity_std": np.std([m["clip_similarity"] for m in metrics_list]),}
    by_type = defaultdict(list)
    for m in metrics_list:
        by_type[m["editing_type_id"]].append(m)
    type_metrics = {}
    for type_id, type_metrics_list in by_type.items():
        type_metrics[type_id] = {
            "type_name": EDITING_TYPE_NAMES.get(type_id, "unknown"),
            "num_images": len(type_metrics_list),
            "ssim_mean": np.mean([m["ssim"] for m in type_metrics_list]),
            "ssim_std": np.std([m["ssim"] for m in type_metrics_list]),
            "lpips_mean": np.mean([m["lpips"] for m in type_metrics_list]),
            "lpips_std": np.std([m["lpips"] for m in type_metrics_list]),
            "clip_similarity_mean": np.mean([m["clip_similarity"] for m in type_metrics_list]),
            "clip_similarity_std": np.std([m["clip_similarity"] for m in type_metrics_list]),}
    return overall, type_metrics

baseline_overall, baseline_by_type = aggregate_metrics(baseline_metrics, "Baseline InstructPix2Pix")
improved_overall, improved_by_type = aggregate_metrics(improved_metrics, "Improved InstructPix2Pix")

print("Metrics aggregated!")

# Aggregating metrics
def aggregate_metrics(metrics_list, method_name):
    overall = {
        "method": method_name,
        "num_images": len(metrics_list),
        "ssim_mean": np.mean([m["ssim"] for m in metrics_list]),
        "ssim_std": np.std([m["ssim"] for m in metrics_list]),
        "lpips_mean": np.mean([m["lpips"] for m in metrics_list]),
        "lpips_std": np.std([m["lpips"] for m in metrics_list]),
        "clip_similarity_mean": np.mean([m["clip_similarity"] for m in metrics_list]),
        "clip_similarity_std": np.std([m["clip_similarity"] for m in metrics_list]),}
    by_type = defaultdict(list)
    for m in metrics_list:
        by_type[m["editing_type_id"]].append(m)
    type_metrics = {}
    for type_id, type_metrics_list in by_type.items():
        type_metrics[type_id] = {
            "type_name": EDITING_TYPE_NAMES.get(type_id, "unknown"),
            "num_images": len(type_metrics_list),
            "ssim_mean": np.mean([m["ssim"] for m in type_metrics_list]),
            "ssim_std": np.std([m["ssim"] for m in type_metrics_list]),
            "lpips_mean": np.mean([m["lpips"] for m in type_metrics_list]),
            "lpips_std": np.std([m["lpips"] for m in type_metrics_list]),
            "clip_similarity_mean": np.mean([m["clip_similarity"] for m in type_metrics_list]),
            "clip_similarity_std": np.std([m["clip_similarity"] for m in type_metrics_list]),}
    return overall, type_metrics

# Baseline + Improved v3
baseline_overall, baseline_by_type = aggregate_metrics(baseline_metrics, "Baseline InstructPix2Pix")
improved3_overall, improved3_by_type = aggregate_metrics(improved3_metrics, "Improved v3 InstructPix2Pix")

print("Metrics aggregated for baseline and improved v3!")

# Aggregating metrics
def aggregate_metrics(metrics_list, method_name):
    overall = {
        "method": method_name,
        "num_images": len(metrics_list),
        "ssim_mean": np.mean([m["ssim"] for m in metrics_list]),
        "ssim_std": np.std([m["ssim"] for m in metrics_list]),
        "lpips_mean": np.mean([m["lpips"] for m in metrics_list]),
        "lpips_std": np.std([m["lpips"] for m in metrics_list]),
        "clip_similarity_mean": np.mean([m["clip_similarity"] for m in metrics_list]),
        "clip_similarity_std": np.std([m["clip_similarity"] for m in metrics_list]),}
    by_type = defaultdict(list)
    for m in metrics_list:
        by_type[m["editing_type_id"]].append(m)
    type_metrics = {}
    for type_id, type_metrics_list in by_type.items():
        type_metrics[type_id] = {
            "type_name": EDITING_TYPE_NAMES.get(type_id, "unknown"),
            "num_images": len(type_metrics_list),
            "ssim_mean": np.mean([m["ssim"] for m in type_metrics_list]),
            "ssim_std": np.std([m["ssim"] for m in type_metrics_list]),
            "lpips_mean": np.mean([m["lpips"] for m in type_metrics_list]),
            "lpips_std": np.std([m["lpips"] for m in type_metrics_list]),
            "clip_similarity_mean": np.mean([m["clip_similarity"] for m in type_metrics_list]),
            "clip_similarity_std": np.std([m["clip_similarity"] for m in type_metrics_list]),}
    return overall, type_metrics

# Baseline + Improved v5
baseline_overall, baseline_by_type = aggregate_metrics(baseline_metrics, "Baseline InstructPix2Pix")
improved5_overall, improved5_by_type = aggregate_metrics(improved5_metrics, "Improved v5 InstructPix2Pix")

print("Metrics aggregated for baseline and improved v5!")

# Baseline overall results
print("=" * 40)
print("OVERALL RESULTS")
print("=" * 40)
print(f"\nBaseline InstructPix2Pix:")
print(f"  SSIM: {baseline_overall['ssim_mean']:.4f} ± {baseline_overall['ssim_std']:.4f}")
print(f"  LPIPS: {baseline_overall['lpips_mean']:.4f} ± {baseline_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {baseline_overall['clip_similarity_mean']:.4f} ± {baseline_overall['clip_similarity_std']:.4f}")

# Saving results to CSV
results_data = []

# Overall results
results_data.append({
    "Method": "Baseline InstructPix2Pix", "Type": "Overall",
    "SSIM_Mean": baseline_overall["ssim_mean"], "SSIM_Std": baseline_overall["ssim_std"],
    "LPIPS_Mean": baseline_overall["lpips_mean"], "LPIPS_Std": baseline_overall["lpips_std"],
    "CLIP_Mean": baseline_overall["clip_similarity_mean"], "CLIP_Std": baseline_overall["clip_similarity_std"],
    "Num_Images": baseline_overall["num_images"]})

# Results by type
for type_id in sorted(baseline_by_type.keys()):
    type_name = baseline_by_type[type_id]["type_name"]
    results_data.append({
        "Method": "Baseline InstructPix2Pix", "Type": type_name,
        "SSIM_Mean": baseline_by_type[type_id]["ssim_mean"], "SSIM_Std": baseline_by_type[type_id]["ssim_std"],
        "LPIPS_Mean": baseline_by_type[type_id]["lpips_mean"], "LPIPS_Std": baseline_by_type[type_id]["lpips_std"],
        "CLIP_Mean": baseline_by_type[type_id]["clip_similarity_mean"], "CLIP_Std": baseline_by_type[type_id]["clip_similarity_std"],
        "Num_Images": baseline_by_type[type_id]["num_images"]})

df_results = pd.DataFrame(results_data)
results_path = "/content/results/evaluation_results.csv"
df_results.to_csv(results_path, index=False)
print(f"\nResults saved to {results_path}")
print("\nResults DataFrame:")
print(df_results.to_string())

print("\n" + "=" * 60)
print("RESULTS BY EDITING TYPE")
print("=" * 60)

common_type_ids = sorted(set(baseline_by_type.keys()) & set(improved_by_type.keys()))

for type_id in common_type_ids:
    type_name = baseline_by_type[type_id]["type_name"]
    print(f"\n{type_name} (Type {type_id}):")
    print(f"  Baseline - SSIM: {baseline_by_type[type_id]['ssim_mean']:.4f}, "
          f"LPIPS: {baseline_by_type[type_id]['lpips_mean']:.4f}, "
          f"CLIP: {baseline_by_type[type_id]['clip_similarity_mean']:.4f}")
    print(f"  Improved - SSIM: {improved_by_type[type_id]['ssim_mean']:.4f}, "
          f"LPIPS: {improved_by_type[type_id]['lpips_mean']:.4f}, "
          f"CLIP: {improved_by_type[type_id]['clip_similarity_mean']:.4f}")

print("\n" + "=" * 60)
print("RESULTS BY EDITING TYPE (v3)")
print("=" * 60)

common_type_ids = sorted(set(baseline_by_type.keys()) & set(improved3_by_type.keys()))

for type_id in common_type_ids:
    type_name = baseline_by_type[type_id]["type_name"]
    print(f"\n{type_name} (Type {type_id}):")
    print(f"  Baseline    - SSIM: {baseline_by_type[type_id]['ssim_mean']:.4f}, "
        f"LPIPS: {baseline_by_type[type_id]['lpips_mean']:.4f}, "
        f"CLIP: {baseline_by_type[type_id]['clip_similarity_mean']:.4f}")
    print(f"  Improved_v3 - SSIM: {improved3_by_type[type_id]['ssim_mean']:.4f}, "
        f"LPIPS: {improved3_by_type[type_id]['lpips_mean']:.4f}, "
        f"CLIP: {improved3_by_type[type_id]['clip_similarity_mean']:.4f}")

print("\n" + "=" * 60)
print("RESULTS BY EDITING TYPE (v5)")
print("=" * 60)

common_type_ids = sorted(set(baseline_by_type.keys()) & set(improved5_by_type.keys()))

for type_id in common_type_ids:
    type_name = baseline_by_type[type_id]["type_name"]
    print(f"\n{type_name} (Type {type_id}):")
    print(f"  Baseline    - SSIM: {baseline_by_type[type_id]['ssim_mean']:.4f}, "
        f"LPIPS: {baseline_by_type[type_id]['lpips_mean']:.4f}, "
        f"CLIP: {baseline_by_type[type_id]['clip_similarity_mean']:.4f}")
    print(f"  Improved_v5 - SSIM: {improved5_by_type[type_id]['ssim_mean']:.4f}, "
        f"LPIPS: {improved5_by_type[type_id]['lpips_mean']:.4f}, "
        f"CLIP: {improved5_by_type[type_id]['clip_similarity_mean']:.4f}")

# Printing overall results
print("=" * 40)
print("OVERALL RESULTS (BASELINE VS IMPROVED)")
print("=" * 40)
print(f"\nBaseline InstructPix2Pix:")
print(f"  SSIM: {baseline_overall['ssim_mean']:.4f} ± {baseline_overall['ssim_std']:.4f}")
print(f"  LPIPS: {baseline_overall['lpips_mean']:.4f} ± {baseline_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {baseline_overall['clip_similarity_mean']:.4f} ± {baseline_overall['clip_similarity_std']:.4f}")

print(f"\nImproved InstructPix2Pix:")
print(f"  SSIM: {improved_overall['ssim_mean']:.4f} ± {improved_overall['ssim_std']:.4f}")
print(f"  LPIPS: {improved_overall['lpips_mean']:.4f} ± {improved_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {improved_overall['clip_similarity_mean']:.4f} ± {improved_overall['clip_similarity_std']:.4f}")

print(f"\nImprovements:")
print(f"  SSIM: {improved_overall['ssim_mean'] - baseline_overall['ssim_mean']:+.4f} (Higher means improved)")
print(f"  LPIPS: {improved_overall['lpips_mean'] - baseline_overall['lpips_mean']:+.4f} (Lower means improved)")
print(f"  CLIP Similarity: {improved_overall['clip_similarity_mean'] - baseline_overall['clip_similarity_mean']:+.4f} (High Better)")

# Printing overall results
print("=" * 40)
print("OVERALL RESULTS (BASELINE VS IMPROVED v3)")
print("=" * 40)

print(f"\nBaseline InstructPix2Pix:")
print(f"  SSIM: {baseline_overall['ssim_mean']:.4f} ± {baseline_overall['ssim_std']:.4f}")
print(f"  LPIPS: {baseline_overall['lpips_mean']:.4f} ± {baseline_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {baseline_overall['clip_similarity_mean']:.4f} ± {baseline_overall['clip_similarity_std']:.4f}")

print(f"\nImproved v3 InstructPix2Pix:")
print(f"  SSIM: {improved3_overall['ssim_mean']:.4f} ± {improved3_overall['ssim_std']:.4f}")
print(f"  LPIPS: {improved3_overall['lpips_mean']:.4f} ± {improved3_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {improved3_overall['clip_similarity_mean']:.4f} ± {improved3_overall['clip_similarity_std']:.4f}")

print(f"\nImprovements (Improved v3 − Baseline):")
print(f"  SSIM: {improved3_overall['ssim_mean'] - baseline_overall['ssim_mean']:+.4f} (Higher means improved)")
print(f"  LPIPS: {improved3_overall['lpips_mean'] - baseline_overall['lpips_mean']:+.4f} (Lower means improved)")
print(f"  CLIP Similarity: {improved3_overall['clip_similarity_mean'] - baseline_overall['clip_similarity_mean']:+.4f} (High Better)")

# Printing overall results
print("=" * 40)
print("OVERALL RESULTS (BASELINE VS IMPROVED v5)")
print("=" * 40)

print(f"\nBaseline InstructPix2Pix:")
print(f"  SSIM: {baseline_overall['ssim_mean']:.4f} ± {baseline_overall['ssim_std']:.4f}")
print(f"  LPIPS: {baseline_overall['lpips_mean']:.4f} ± {baseline_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {baseline_overall['clip_similarity_mean']:.4f} ± {baseline_overall['clip_similarity_std']:.4f}")

print(f"\nImproved v5 InstructPix2Pix:")
print(f"  SSIM: {improved5_overall['ssim_mean']:.4f} ± {improved5_overall['ssim_std']:.4f}")
print(f"  LPIPS: {improved5_overall['lpips_mean']:.4f} ± {improved5_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {improved5_overall['clip_similarity_mean']:.4f} ± {improved5_overall['clip_similarity_std']:.4f}")

print(f"\nImprovements (Improved v5 − Baseline):")
print(f"  SSIM: {improved5_overall['ssim_mean'] - baseline_overall['ssim_mean']:+.4f} (Higher means improved)")
print(f"  LPIPS: {improved5_overall['lpips_mean'] - baseline_overall['lpips_mean']:+.4f} (Lower means improved)")
print(f"  CLIP Similarity: {improved5_overall['clip_similarity_mean'] - baseline_overall['clip_similarity_mean']:+.4f} (High Better)")

print("Baseline results:", len(baseline_results))
print("Improved results:", len(improved_results))
print("Baseline metrics:", len(baseline_metrics))
print("Improved metrics:", len(improved_metrics))

print("Baseline results:", len(baseline_results))
print("Improved_v5 results:", len(improved5_results))
print("Baseline metrics:", len(baseline_metrics))
print("Improved_v5 metrics:", len(improved5_metrics))

from PIL import Image
import numpy as np

k = baseline_results[0]["key"]
b_path = [r["output_path"] for r in baseline_results if r["key"] == k][0]
i_path = [r["output_path"] for r in improved_results if r["key"] == k][0]

b_img = np.array(Image.open(b_path).convert("RGB"))
i_img = np.array(Image.open(i_path).convert("RGB"))

print("Shape baseline:", b_img.shape, "Shape improved:", i_img.shape)
print("Max abs diff:", np.abs(b_img.astype(np.int16) - i_img.astype(np.int16)).max())

from PIL import Image
import numpy as np

k = baseline_results[0]["key"]
b_path = [r["output_path"] for r in baseline_results if r["key"] == k][0]
i_path = [r["output_path"] for r in improved3_results if r["key"] == k][0]

b_img = np.array(Image.open(b_path).convert("RGB"))
i_img = np.array(Image.open(i_path).convert("RGB"))

print("Shape baseline:", b_img.shape, "Shape improved:", i_img.shape)
print("Max abs diff:", np.abs(b_img.astype(np.int16) - i_img.astype(np.int16)).max())

# Improved v5 version
from PIL import Image
import numpy as np

k = baseline_results[0]["key"]
b_path = [r["output_path"] for r in baseline_results if r["key"] == k][0]
i_path = [r["output_path"] for r in improved5_results if r["key"] == k][0]

b_img = np.array(Image.open(b_path).convert("RGB"))
i_img = np.array(Image.open(i_path).convert("RGB"))

print("Shape baseline:", b_img.shape, "Shape improved:", i_img.shape)
print("Max abs diff:", np.abs(b_img.astype(np.int16) - i_img.astype(np.int16)).max())

results_data = []

# Overall results
results_data.append({
    "Method": "Baseline InstructPix2Pix", "Type": "Overall",
    "SSIM_Mean": baseline_overall["ssim_mean"], "SSIM_Std": baseline_overall["ssim_std"],
    "LPIPS_Mean": baseline_overall["lpips_mean"], "LPIPS_Std": baseline_overall["lpips_std"],
    "CLIP_Mean": baseline_overall["clip_similarity_mean"], "CLIP_Std": baseline_overall["clip_similarity_std"],
    "Num_Images": baseline_overall["num_images"]
})

results_data.append({
    "Method": "Improved InstructPix2Pix", "Type": "Overall",
    "SSIM_Mean": improved_overall["ssim_mean"], "SSIM_Std": improved_overall["ssim_std"],
    "LPIPS_Mean": improved_overall["lpips_mean"], "LPIPS_Std": improved_overall["lpips_std"],
    "CLIP_Mean": improved_overall["clip_similarity_mean"], "CLIP_Std": improved_overall["clip_similarity_std"],
    "Num_Images": improved_overall["num_images"]
})

# Results by type
for type_id in sorted(baseline_by_type.keys()):
    type_name = baseline_by_type[type_id]["type_name"]
    results_data.append({
        "Method": "Baseline InstructPix2Pix", "Type": type_name,
        "SSIM_Mean": baseline_by_type[type_id]["ssim_mean"], "SSIM_Std": baseline_by_type[type_id]["ssim_std"],
        "LPIPS_Mean": baseline_by_type[type_id]["lpips_mean"], "LPIPS_Std": baseline_by_type[type_id]["lpips_std"],
        "CLIP_Mean": baseline_by_type[type_id]["clip_similarity_mean"], "CLIP_Std": baseline_by_type[type_id]["clip_similarity_std"],
        "Num_Images": baseline_by_type[type_id]["num_images"]
    })
    results_data.append({
        "Method": "Improved InstructPix2Pix", "Type": type_name,
        "SSIM_Mean": improved_by_type[type_id]["ssim_mean"], "SSIM_Std": improved_by_type[type_id]["ssim_std"],
        "LPIPS_Mean": improved_by_type[type_id]["lpips_mean"], "LPIPS_Std": improved_by_type[type_id]["lpips_std"],
        "CLIP_Mean": improved_by_type[type_id]["clip_similarity_mean"], "CLIP_Std": improved_by_type[type_id]["clip_similarity_std"],
        "Num_Images": improved_by_type[type_id]["num_images"]
    })

df_results = pd.DataFrame(results_data)
results_path = "/content/results/evaluation_results.csv"
df_results.to_csv(results_path, index=False)
print(f"\nResults saved to {results_path}")
print("\nResults DataFrame:")
print(df_results.to_string())

results3_data = []

# Overall results (v3)
results3_data.append({
    "Method": "Baseline InstructPix2Pix",
    "Type": "Overall",
    "SSIM_Mean": baseline_overall["ssim_mean"],
    "SSIM_Std": baseline_overall["ssim_std"],
    "LPIPS_Mean": baseline_overall["lpips_mean"],
    "LPIPS_Std": baseline_overall["lpips_std"],
    "CLIP_Mean": baseline_overall["clip_similarity_mean"],
    "CLIP_Std": baseline_overall["clip_similarity_std"],
    "Num_Images": baseline_overall["num_images"],
})

results3_data.append({
    "Method": "Improved v3 InstructPix2Pix",
    "Type": "Overall",
    "SSIM_Mean": improved3_overall["ssim_mean"],
    "SSIM_Std": improved3_overall["ssim_std"],
    "LPIPS_Mean": improved3_overall["lpips_mean"],
    "LPIPS_Std": improved3_overall["lpips_std"],
    "CLIP_Mean": improved3_overall["clip_similarity_mean"],
    "CLIP_Std": improved3_overall["clip_similarity_std"],
    "Num_Images": improved3_overall["num_images"],
})

# Results by type (v3)
for type_id in sorted(baseline_by_type.keys()):
    type_name = baseline_by_type[type_id]["type_name"]

    results3_data.append({
        "Method": "Baseline InstructPix2Pix",
        "Type": type_name,
        "SSIM_Mean": baseline_by_type[type_id]["ssim_mean"],
        "SSIM_Std": baseline_by_type[type_id]["ssim_std"],
        "LPIPS_Mean": baseline_by_type[type_id]["lpips_mean"],
        "LPIPS_Std": baseline_by_type[type_id]["lpips_std"],
        "CLIP_Mean": baseline_by_type[type_id]["clip_similarity_mean"],
        "CLIP_Std": baseline_by_type[type_id]["clip_similarity_std"],
        "Num_Images": baseline_by_type[type_id]["num_images"],
    })

    results3_data.append({
        "Method": "Improved v3 InstructPix2Pix",
        "Type": type_name,
        "SSIM_Mean": improved3_by_type[type_id]["ssim_mean"],
        "SSIM_Std": improved3_by_type[type_id]["ssim_std"],
        "LPIPS_Mean": improved3_by_type[type_id]["lpips_mean"],
        "LPIPS_Std": improved3_by_type[type_id]["lpips_std"],
        "CLIP_Mean": improved3_by_type[type_id]["clip_similarity_mean"],
        "CLIP_Std": improved3_by_type[type_id]["clip_similarity_std"],
        "Num_Images": improved3_by_type[type_id]["num_images"],
    })

df_results3 = pd.DataFrame(results3_data)
results_path3 = "/content/results/evaluation_results_v3.csv"
df_results3.to_csv(results_path3, index=False)

print(f"\nResults (v3) saved to {results_path3}")
print("\nResults DataFrame (v3):")
print(df_results3.to_string())

results5_data = []

# Overall results (v5)
results5_data.append({
    "Method": "Baseline InstructPix2Pix",
    "Type": "Overall",
    "SSIM_Mean": baseline_overall["ssim_mean"],
    "SSIM_Std": baseline_overall["ssim_std"],
    "LPIPS_Mean": baseline_overall["lpips_mean"],
    "LPIPS_Std": baseline_overall["lpips_std"],
    "CLIP_Mean": baseline_overall["clip_similarity_mean"],
    "CLIP_Std": baseline_overall["clip_similarity_std"],
    "Num_Images": baseline_overall["num_images"],
})

results5_data.append({
    "Method": "Improved v5 InstructPix2Pix",
    "Type": "Overall",
    "SSIM_Mean": improved5_overall["ssim_mean"],
    "SSIM_Std": improved5_overall["ssim_std"],
    "LPIPS_Mean": improved5_overall["lpips_mean"],
    "LPIPS_Std": improved5_overall["lpips_std"],
    "CLIP_Mean": improved5_overall["clip_similarity_mean"],
    "CLIP_Std": improved5_overall["clip_similarity_std"],
    "Num_Images": improved5_overall["num_images"],
})

# Results by type (v5)
for type_id in sorted(baseline_by_type.keys()):
    type_name = baseline_by_type[type_id]["type_name"]

    results5_data.append({
        "Method": "Baseline InstructPix2Pix",
        "Type": type_name,
        "SSIM_Mean": baseline_by_type[type_id]["ssim_mean"],
        "SSIM_Std": baseline_by_type[type_id]["ssim_std"],
        "LPIPS_Mean": baseline_by_type[type_id]["lpips_mean"],
        "LPIPS_Std": baseline_by_type[type_id]["lpips_std"],
        "CLIP_Mean": baseline_by_type[type_id]["clip_similarity_mean"],
        "CLIP_Std": baseline_by_type[type_id]["clip_similarity_std"],
        "Num_Images": baseline_by_type[type_id]["num_images"],
    })

    results5_data.append({
        "Method": "Improved v5 InstructPix2Pix",
        "Type": type_name,
        "SSIM_Mean": improved5_by_type[type_id]["ssim_mean"],
        "SSIM_Std": improved5_by_type[type_id]["ssim_std"],
        "LPIPS_Mean": improved5_by_type[type_id]["lpips_mean"],
        "LPIPS_Std": improved5_by_type[type_id]["lpips_std"],
        "CLIP_Mean": improved5_by_type[type_id]["clip_similarity_mean"],
        "CLIP_Std": improved5_by_type[type_id]["clip_similarity_std"],
        "Num_Images": improved5_by_type[type_id]["num_images"],
    })

df_results5 = pd.DataFrame(results5_data)
results_path5 = "/content/results/evaluation_results_v5.csv"
df_results5.to_csv(results_path5, index=False)

print(f"\nResults (v5) saved to {results_path5}")
print("\nResults DataFrame (v5):")
print(df_results5.to_string())

"""## 11. Visualization

"""

df_types = df_results[df_results["Type"] != "Overall"].copy()

# Separating baseline vs improved and index by type
baseline_df = df_types[df_types["Method"] == "Baseline InstructPix2Pix"].set_index("Type")
improved_df = df_types[df_results["Method"] == "Improved InstructPix2Pix"].set_index("Type")

# Editing types as ordered list
types = baseline_df.index.tolist()
x = np.arange(len(types))

print("Types:", types)
print("Baseline rows:", len(baseline_df), "Improved rows:", len(improved_df))

fig, axes = plt.subplots(1,3,figsize=(18,4),constrained_layout=False)

metrics = [("SSIM_Mean", "SSIM"),
    ("LPIPS_Mean", "LPIPS (↓ better)"),
    ("CLIP_Mean", "CLIP Similarity"),]

width = 0.28
for ax, (metric_col, title) in zip(axes, metrics):
    baseline_vals = baseline_df[metric_col].values
    improved_vals = improved_df[metric_col].values

    ax.bar(x - width/2, baseline_vals, width, label="Baseline", alpha=0.8)
    ax.bar(x + width/2, improved_vals, width, label="Improved", alpha=0.8)

    ax.set_xticks(x)
    ax.set_xticklabels(types, rotation=35, ha="right", fontsize=8)
    ax.set_title(title, fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.tick_params(axis="y", labelsize=8)

axes[0].set_ylabel("Score", fontsize=10)
axes[0].legend(loc="upper left", fontsize=8)

fig.suptitle("Baseline vs Improved by Editing Type", fontsize=14, y=0.98)
fig.tight_layout(rect=[0, 0, 1, 0.99])

plt.savefig("/content/results/metrics_comparison1.png",dpi=300,bbox_inches="tight")
plt.show()

print("Saved bar plot to /content/results/metrics_comparison1.png")

fig, axes = plt.subplots(1,3,figsize=(18,4),constrained_layout=False)

metrics = [
    ("SSIM_Mean", "ΔSSIM (Improved − Baseline)"),
    ("LPIPS_Mean", "ΔLPIPS (Improved − Baseline, ↓ better)"),
    ("CLIP_Mean", "ΔCLIP (Improved − Baseline)"),]

for ax, (metric_col, title) in zip(axes, metrics):
    delta = improved_df[metric_col] - baseline_df[metric_col]

    ax.axhline(0.0, linestyle="--", linewidth=1, color="gray")
    ax.bar(x, delta.values, width=0.5, alpha=0.85, color="seagreen")

    if len(delta) > 0:
        d_min = float(np.nanmin(delta.values))
        d_max = float(np.nanmax(delta.values))
        if d_min == d_max:
            d_min -= 0.1
            d_max += 0.1
        margin = 0.1 * (d_max - d_min)
        y_min = d_min - margin
        y_max = d_max + margin

        y_min = min(y_min, -0.05)
        y_max = max(y_max,  0.05)
        ax.set_ylim(y_min, y_max)

    ax.set_xticks(x)
    ax.set_xticklabels(types, rotation=35, ha="right", fontsize=8)
    ax.set_title(title, fontsize=11)
    ax.grid(True, axis="y", alpha=0.3)
    ax.tick_params(axis="y", labelsize=8)

    y_min, y_max = ax.get_ylim()
    offset = 0.02 * (y_max - y_min)
    for xi, d in zip(x, delta.values):
        if np.isnan(d):
            continue
        va = "bottom" if d >= 0 else "top"
        y_text = d + (offset if d >= 0 else -offset)
        ax.text(xi, y_text, f"{d:.3f}", ha="center", va=va, fontsize=7)

fig.suptitle("Per-Type Metric Improvements (Improved - Baseline)",fontsize=14,y=0.98)
fig.tight_layout(rect=[0, 0, 1, 0.99])

plt.savefig("/content/results/metrics_deltas.png", dpi=300, bbox_inches="tight")
plt.show()

print("Saved delta plot to /content/results/metrics_deltas.png")

import numpy as np
import matplotlib.pyplot as plt

overall_df = df_results[df_results["Type"] == "Overall"].set_index("Method")
baseline_overall = overall_df.loc["Baseline InstructPix2Pix"]
improved_overall = overall_df.loc["Improved InstructPix2Pix"]

print("Baseline overall:")
print("  SSIM  =", baseline_overall["SSIM_Mean"])
print("  LPIPS =", baseline_overall["LPIPS_Mean"])
print("  CLIP  =", baseline_overall["CLIP_Mean"])
print("\nImproved overall:")
print("  SSIM  =", improved_overall["SSIM_Mean"])
print("  LPIPS =", improved_overall["LPIPS_Mean"])
print("  CLIP  =", improved_overall["CLIP_Mean"])

baseline_scores = np.array([
    baseline_overall["SSIM_Mean"], # SSIM
    1.0 - baseline_overall["LPIPS_Mean"], # 1 - LPIPS
    baseline_overall["CLIP_Mean"] / 30.0, # CLIP scaled
])

improved_scores = np.array([
    improved_overall["SSIM_Mean"],
    1.0 - improved_overall["LPIPS_Mean"],
    improved_overall["CLIP_Mean"] / 30.0,])

labels = ["SSIM", "1 − LPIPS", "CLIP / 30"]

angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False)
angles = np.concatenate([angles, [angles[0]]])
baseline_scores = np.concatenate([baseline_scores, [baseline_scores[0]]])
improved_scores = np.concatenate([improved_scores, [improved_scores[0]]])

all_scores = np.concatenate([baseline_scores, improved_scores])
s_min = float(all_scores.min())
s_max = float(all_scores.max())
margin = 0.05 * (s_max - s_min)

r_min = max(0.0, s_min - margin)
r_max = min(1.0, s_max + margin)

print("\nRadar radial limits:", r_min, "to", r_max)

fig = plt.figure(figsize=(5, 5))
ax = fig.add_subplot(111, polar=True)

ax.plot(angles, baseline_scores, marker="o", label="Baseline", linewidth=2)
ax.fill(angles, baseline_scores, alpha=0.25)

ax.plot(angles, improved_scores, marker="o", label="Improved", linewidth=2)
ax.fill(angles, improved_scores, alpha=0.25)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(labels, fontsize=10)

ax.set_ylim(r_min, r_max)

ax.set_title("Overall Metrics (Normalized Radar Plot)", fontsize=14, y=1.12)
ax.legend(loc="upper right", bbox_to_anchor=(1.25, 1.1), fontsize=9)

plt.tight_layout()
plt.savefig("/content/results/overall_radar.png", dpi=300, bbox_inches="tight")
plt.show()

print("Saved radar plot to /content/results/overall_radar.png")

FOLDER_PATH = "/content/checkpoints"
ZIP_PATH = "/content/checkpoints_zip.zip"

!zip -r "$ZIP_PATH" "$FOLDER_PATH"

# Downloading the zip file
from google.colab import files
files.download(ZIP_PATH)

df_types3 = df_results3[df_results3["Type"] != "Overall"].copy()

baseline_df_v3 = df_types3[df_types3["Method"] == "Baseline InstructPix2Pix"].set_index("Type")
improved3_df   = df_types3[df_types3["Method"] == "Improved v3 InstructPix2Pix"].set_index("Type")

types = baseline_df_v3.index.tolist()
x = np.arange(len(types))

print("Types:", types)
print("Baseline rows:", len(baseline_df_v3), "Improved v3 rows:", len(improved3_df))

fig, axes = plt.subplots(1,3,figsize=(18,4),constrained_layout=False)

metrics = [("SSIM_Mean", "SSIM"),
    ("LPIPS_Mean", "LPIPS (↓ better)"),
    ("CLIP_Mean", "CLIP Similarity"),]

width = 0.28
for ax, (metric_col, title) in zip(axes, metrics):
    baseline_vals = baseline_df_v3[metric_col].values
    improved_vals = improved3_df[metric_col].values

    ax.bar(x - width/2, baseline_vals, width, label="Baseline", alpha=0.8)
    ax.bar(x + width/2, improved_vals, width, label="Improved v3", alpha=0.8)

    ax.set_xticks(x)
    ax.set_xticklabels(types, rotation=35, ha="right", fontsize=8)
    ax.set_title(title, fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.tick_params(axis="y", labelsize=8)

axes[0].set_ylabel("Score", fontsize=10)
axes[0].legend(loc="upper left", fontsize=8)

fig.suptitle("Baseline vs Improved v3 by Editing Type", fontsize=14, y=0.98)
fig.tight_layout(rect=[0, 0, 1, 0.99])

plt.savefig("/content/results/metrics_comparison1_v3.png",dpi=300,bbox_inches="tight")
plt.show()

print("Saved bar plot to /content/results/metrics_comparison1_v3.png")

fig, axes = plt.subplots(1,3,figsize=(18,4),constrained_layout=False)

metrics = [
    ("SSIM_Mean", "ΔSSIM (Improved v3 − Baseline)"),
    ("LPIPS_Mean", "ΔLPIPS (Improved v3 − Baseline, ↓ better)"),
    ("CLIP_Mean", "ΔCLIP (Improved v3 − Baseline)"),]

for ax, (metric_col, title) in zip(axes, metrics):
    delta = improved3_df[metric_col] - baseline_df_v3[metric_col]

    ax.axhline(0.0, linestyle="--", linewidth=1, color="gray")
    ax.bar(x, delta.values, width=0.5, alpha=0.85, color="seagreen")

    if len(delta) > 0:
        d_min = float(np.nanmin(delta.values))
        d_max = float(np.nanmax(delta.values))
        if d_min == d_max:
            d_min -= 0.1
            d_max += 0.1
        margin = 0.1 * (d_max - d_min)
        y_min = d_min - margin
        y_max = d_max + margin

        y_min = min(y_min, -0.05)
        y_max = max(y_max,  0.05)
        ax.set_ylim(y_min, y_max)

    ax.set_xticks(x)
    ax.set_xticklabels(types, rotation=35, ha="right", fontsize=8)
    ax.set_title(title, fontsize=11)
    ax.grid(True, axis="y", alpha=0.3)
    ax.tick_params(axis="y", labelsize=8)

    y_min, y_max = ax.get_ylim()
    offset = 0.02 * (y_max - y_min)
    for xi, d in zip(x, delta.values):
        if np.isnan(d):
            continue
        va = "bottom" if d >= 0 else "top"
        y_text = d + (offset if d >= 0 else -offset)
        ax.text(xi, y_text, f"{d:.3f}", ha="center", va=va, fontsize=7)

fig.suptitle("Per-Type Metric Improvements (Improved v3 - Baseline)",fontsize=14,y=0.98)
fig.tight_layout(rect=[0, 0, 1, 0.99])

plt.savefig("/content/results/metrics_deltas_v3.png", dpi=300, bbox_inches="tight")
plt.show()

print("Saved delta plot to /content/results/metrics_deltas_v3.png")

overall_df3 = df_results3[df_results3["Type"] == "Overall"].set_index("Method")
baseline_overall = overall_df3.loc["Baseline InstructPix2Pix"]
improved3_overall = overall_df3.loc["Improved v3 InstructPix2Pix"]

print("Baseline overall:")
print("  SSIM  =", baseline_overall["SSIM_Mean"])
print("  LPIPS =", baseline_overall["LPIPS_Mean"])
print("  CLIP  =", baseline_overall["CLIP_Mean"])

print("\nImproved v3 overall:")
print("  SSIM  =", improved3_overall["SSIM_Mean"])
print("  LPIPS =", improved3_overall["LPIPS_Mean"])
print("  CLIP  =", improved3_overall["CLIP_Mean"])

baseline_scores = np.array([
    baseline_overall["SSIM_Mean"], # SSIM
    1.0 - baseline_overall["LPIPS_Mean"], # 1 - LPIPS
    baseline_overall["CLIP_Mean"] / 30.0, # CLIP scaled (/30)
])

improved_scores = np.array([
    improved3_overall["SSIM_Mean"],
    1.0 - improved3_overall["LPIPS_Mean"],
    improved3_overall["CLIP_Mean"] / 30.0,])

labels = ["SSIM", "1 − LPIPS", "CLIP / 30"]

angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False)
angles = np.concatenate([angles, [angles[0]]])
baseline_scores = np.concatenate([baseline_scores, [baseline_scores[0]]])
improved_scores = np.concatenate([improved_scores, [improved_scores[0]]])

all_scores = np.concatenate([baseline_scores, improved_scores])
s_min = float(all_scores.min())
s_max = float(all_scores.max())
margin = 0.05 * (s_max - s_min)

r_min = max(0.0, s_min - margin)
r_max = min(1.0, s_max + margin)

print("\nRadar radial limits:", r_min, "to", r_max)

fig = plt.figure(figsize=(5, 5))
ax = fig.add_subplot(111, polar=True)

ax.plot(angles, baseline_scores, marker="o", label="Baseline", linewidth=2)
ax.fill(angles, baseline_scores, alpha=0.25)

ax.plot(angles, improved_scores, marker="o", label="Improved v3", linewidth=2)
ax.fill(angles, improved_scores, alpha=0.25)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(labels, fontsize=10)

ax.set_ylim(r_min, r_max)

ax.set_title("Overall Metrics (Normalized Radar Plot v3)", fontsize=14, y=1.12)
ax.legend(loc="upper right", bbox_to_anchor=(1.25, 1.1), fontsize=9)

plt.tight_layout()
plt.savefig("/content/results/overall_radar_v3.png", dpi=300, bbox_inches="tight")
plt.show()

print("Saved radar plot to /content/results/overall_radar_v3.png")

df_types5 = df_results5[df_results5["Type"] != "Overall"].copy()

baseline_df_v5 = df_types5[df_types5["Method"] == "Baseline InstructPix2Pix"].set_index("Type")
improved5_df   = df_types5[df_types5["Method"] == "Improved v5 InstructPix2Pix"].set_index("Type")

types = baseline_df_v5.index.tolist()
x = np.arange(len(types))

print("Types:", types)
print("Baseline rows:", len(baseline_df_v5), "Improved v5 rows:", len(improved5_df))

fig, axes = plt.subplots(1,3,figsize=(18,4),constrained_layout=False)

metrics = [("SSIM_Mean", "SSIM"),
    ("LPIPS_Mean", "LPIPS (↓ better)"),
    ("CLIP_Mean", "CLIP Similarity"),]

width = 0.28
for ax, (metric_col, title) in zip(axes, metrics):
    baseline_vals = baseline_df_v5[metric_col].values
    improved_vals = improved5_df[metric_col].values

    ax.bar(x - width/2, baseline_vals, width, label="Baseline", alpha=0.8)
    ax.bar(x + width/2, improved_vals, width, label="Improved v5", alpha=0.8)

    ax.set_xticks(x)
    ax.set_xticklabels(types, rotation=35, ha="right", fontsize=8)
    ax.set_title(title, fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.tick_params(axis="y", labelsize=8)

axes[0].set_ylabel("Score", fontsize=10)
axes[0].legend(loc="upper left", fontsize=8)

fig.suptitle("Baseline vs Improved v5 by Editing Type", fontsize=14, y=0.98)
fig.tight_layout(rect=[0, 0, 1, 0.99])

plt.savefig("/content/results/metrics_comparison1_v5.png",dpi=300,bbox_inches="tight")
plt.show()

print("Saved bar plot to /content/results/metrics_comparison1_v5.png")

fig, axes = plt.subplots(1,3,figsize=(18,4),constrained_layout=False)

metrics = [
    ("SSIM_Mean", "ΔSSIM (Improved v5 − Baseline)"),
    ("LPIPS_Mean", "ΔLPIPS (Improved v5 − Baseline, ↓ better)"),
    ("CLIP_Mean", "ΔCLIP (Improved v5 − Baseline)"),]

for ax, (metric_col, title) in zip(axes, metrics):
    delta = improved5_df[metric_col] - baseline_df_v5[metric_col]

    ax.axhline(0.0, linestyle="--", linewidth=1, color="gray")
    ax.bar(x, delta.values, width=0.5, alpha=0.85, color="seagreen")

    if len(delta) > 0:
        d_min = float(np.nanmin(delta.values))
        d_max = float(np.nanmax(delta.values))
        if d_min == d_max:
            d_min -= 0.1
            d_max += 0.1
        margin = 0.1 * (d_max - d_min)
        y_min = d_min - margin
        y_max = d_max + margin

        y_min = min(y_min, -0.05)
        y_max = max(y_max,  0.05)
        ax.set_ylim(y_min, y_max)

    ax.set_xticks(x)
    ax.set_xticklabels(types, rotation=35, ha="right", fontsize=8)
    ax.set_title(title, fontsize=11)
    ax.grid(True, axis="y", alpha=0.3)
    ax.tick_params(axis="y", labelsize=8)

    y_min, y_max = ax.get_ylim()
    offset = 0.02 * (y_max - y_min)
    for xi, d in zip(x, delta.values):
        if np.isnan(d):
            continue
        va = "bottom" if d >= 0 else "top"
        y_text = d + (offset if d >= 0 else -offset)
        ax.text(xi, y_text, f"{d:.3f}", ha="center", va=va, fontsize=7)

fig.suptitle("Per-Type Metric Improvements (Improved v5 - Baseline)",fontsize=14,y=0.98)
fig.tight_layout(rect=[0, 0, 1, 0.99])

plt.savefig("/content/results/metrics_deltas_v5.png", dpi=300, bbox_inches="tight")
plt.show()

print("Saved delta plot to /content/results/metrics_deltas_v5.png")

overall_df5 = df_results5[df_results5["Type"] == "Overall"].set_index("Method")
baseline_overall = overall_df5.loc["Baseline InstructPix2Pix"]
improved5_overall = overall_df5.loc["Improved v5 InstructPix2Pix"]

print("Baseline overall:")
print("  SSIM  =", baseline_overall["SSIM_Mean"])
print("  LPIPS =", baseline_overall["LPIPS_Mean"])
print("  CLIP  =", baseline_overall["CLIP_Mean"])

print("\nImproved v5 overall:")
print("  SSIM  =", improved5_overall["SSIM_Mean"])
print("  LPIPS =", improved5_overall["LPIPS_Mean"])
print("  CLIP  =", improved5_overall["CLIP_Mean"])

baseline_scores = np.array([
    baseline_overall["SSIM_Mean"], # SSIM
    1.0 - baseline_overall["LPIPS_Mean"], # 1 - LPIPS
    baseline_overall["CLIP_Mean"] / 30.0, # CLIP scaled (/30)
])

improved_scores = np.array([
    improved5_overall["SSIM_Mean"],
    1.0 - improved5_overall["LPIPS_Mean"],
    improved5_overall["CLIP_Mean"] / 30.0,])

labels = ["SSIM", "1 − LPIPS", "CLIP / 30"]

angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False)
angles = np.concatenate([angles, [angles[0]]])
baseline_scores = np.concatenate([baseline_scores, [baseline_scores[0]]])
improved_scores = np.concatenate([improved_scores, [improved_scores[0]]])

all_scores = np.concatenate([baseline_scores, improved_scores])
s_min = float(all_scores.min())
s_max = float(all_scores.max())
margin = 0.05 * (s_max - s_min)

r_min = max(0.0, s_min - margin)
r_max = min(1.0, s_max + margin)

print("\nRadar radial limits:", r_min, "to", r_max)

fig = plt.figure(figsize=(5, 5))
ax = fig.add_subplot(111, polar=True)

ax.plot(angles, baseline_scores, marker="o", label="Baseline", linewidth=2)
ax.fill(angles, baseline_scores, alpha=0.25)

ax.plot(angles, improved_scores, marker="o", label="Improved v5", linewidth=2)
ax.fill(angles, improved_scores, alpha=0.25)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(labels, fontsize=10)

ax.set_ylim(r_min, r_max)

ax.set_title("Overall Metrics (Normalized Radar Plot v5)", fontsize=14, y=1.12)
ax.legend(loc="upper right", bbox_to_anchor=(1.25, 1.1), fontsize=9)

plt.tight_layout()
plt.savefig("/content/results/overall_radar_v5.png", dpi=300, bbox_inches="tight")
plt.show()

print("Saved radar plot to /content/results/overall_radar_v5.png")

"""## 12. Qualitative Example 1: Baseline vs Improved"""

import os
from google.colab import files
from PIL import Image
import matplotlib.pyplot as plt

# Uploading the example image
print("Upload the portrait image...")
uploaded = files.upload()

example_image_path = list(uploaded.keys())[0]
print("Using image:", example_image_path)

# Defining the edit prompt and type id
edit_prompt = ("Add a Christmas Santa hat on the man's head with a smile. "
    "Keep his face, hairstyle, pose, clothing, and background the same.")
editing_type_id = "2" # add_object

# Running Baseline InstructPix2Pix
baseline_edited = edit_instruct_pix2pix_baseline(
    baseline_model, example_image_path, edit_prompt, resolution=512,
    steps=50, cfg_text=7.5, cfg_image=1.5, seed=42,)

# Running Improved v3
improved_edited = edit_instruct_pix2pix_improved3(
    baseline_model, example_image_path, edit_prompt,
    editing_type_id, resolution=512, seed=42,)

# Showing Input vs Baseline vs Improved v3
input_image = Image.open(example_image_path).convert("RGB")

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.imshow(input_image)
plt.title("Input")
plt.axis("off")

plt.subplot(1, 3, 2)
plt.imshow(baseline_edited)
plt.title("Baseline InstructPix2Pix")
plt.axis("off")

plt.subplot(1, 3, 3)
plt.imshow(improved_edited)
plt.title("Improved InstructPix2Pix")
plt.axis("off")

plt.tight_layout()
plt.show()

"""## Qualitative Example 2: Baseline vs Improved"""

import os
from google.colab import files
from PIL import Image
import matplotlib.pyplot as plt

# Uploading the example image
print("Upload the portrait image...")
uploaded = files.upload()

example_image_path = list(uploaded.keys())[0]
print("Using image:", example_image_path)

# Defining the edit prompt and type id
edit_prompt = ("Make the man smile with visible teeth. "
    "Keep his face, hair, suit, and background the same.")
editing_type_id = "4" # change_attribute_content

# Running Baseline InstructPix2Pix
baseline_edited = edit_instruct_pix2pix_baseline(
    baseline_model, example_image_path, edit_prompt, resolution=512,
    steps=50, cfg_text=7.5, cfg_image=1.5, seed=42,)

# Running Improved v3
improved_edited = edit_instruct_pix2pix_improved3(
    baseline_model, example_image_path, edit_prompt,
    editing_type_id, resolution=512, seed=42,)

# Showing Input vs Baseline vs Improved v3
input_image = Image.open(example_image_path).convert("RGB")

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.imshow(input_image)
plt.title("Input")
plt.axis("off")

plt.subplot(1, 3, 2)
plt.imshow(baseline_edited)
plt.title("Baseline InstructPix2Pix")
plt.axis("off")

plt.subplot(1, 3, 3)
plt.imshow(improved_edited)
plt.title("Improved InstructPix2Pix")
plt.axis("off")

plt.tight_layout()
plt.show()

"""## Qualitative Example 3: Baseline vs Improved"""

import os
from google.colab import files
from PIL import Image
import matplotlib.pyplot as plt

# Uploading the example image
print("Upload the portrait image...")
uploaded = files.upload()

example_image_path = list(uploaded.keys())[0]
print("Using image:", example_image_path)

edit_prompt = ("Replace the man's eyeglasses with stylish dark sunglasses.")
editing_type_id = "1" # change_object

# Running Baseline InstructPix2Pix
baseline_edited = edit_instruct_pix2pix_baseline(
    baseline_model, example_image_path, edit_prompt, resolution=512,
    steps=50, cfg_text=7.5, cfg_image=1.5, seed=42,)

# Running Improved v3
improved_edited = edit_instruct_pix2pix_improved3(
    baseline_model, example_image_path, edit_prompt,
    editing_type_id, resolution=512, seed=42,)

# Showing Input vs Baseline vs Improved v3
input_image = Image.open(example_image_path).convert("RGB")

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.imshow(input_image)
plt.title("Input")
plt.axis("off")

plt.subplot(1, 3, 2)
plt.imshow(baseline_edited)
plt.title("Baseline InstructPix2Pix")
plt.axis("off")

plt.subplot(1, 3, 3)
plt.imshow(improved_edited)
plt.title("Improved InstructPix2Pix")
plt.axis("off")

plt.tight_layout()
plt.show()

"""## Appendix - Improved v2 InstructPix2Pix"""

# Improved v2 InstructPix2Pix: 2-Stage Type-Aware CFG + Blending
# Builds on my previous "Improved" but:
#   - uses a 2-stage guidance schedule per edit type
#   - keeps structure early (strong image CFG), then pushes semantics
#   - tweaks per-type hyper-params to target the types that regressed

class AdaptiveEditConfigV2:
    """
    Per-edit-type configuration for Improved v2:
      - steps: total diffusion steps
      - split_ratio: fraction of steps for stage 1 (structure-preserving)
      - cfg1_text, cfg1_image: guidance scales for stage 1
      - cfg2_text, cfg2_image: guidance scales for stage 2 (more semantic)
      - blend_alpha: how strong the edited pixels are in the final blend
    """
    EDITING_TYPE_CONFIGS = {
    # Heuristics based on our v1 metrics:
    # - change_object, add_object, change_style got worse LPIPS/SSIM → more structure
    # - color/material/background lost CLIP → slightly stronger late text guidance

        "0": {
            "steps": 50,
            "split_ratio": 0.6,
            "cfg1_text": 7.0, "cfg1_image": 1.8, # preserving structure first
            "cfg2_text": 8.7, "cfg2_image": 1.3, # then stronger semantics
            "blend_alpha": 0.70,
        },

        "1": {
            "steps": 55,
            "split_ratio": 0.65,
            "cfg1_text": 7.0, "cfg1_image": 2.1, # strong image guidance early
            "cfg2_text": 8.5, "cfg2_image": 1.3, # moderate text push
            "blend_alpha": 0.68, # keeping a bit more of original pixels
        },

        "2": {
            "steps": 55,
            "split_ratio": 0.65,
            "cfg1_text": 7.2, "cfg1_image": 2.0, # strong structure
            "cfg2_text": 8.8, "cfg2_image": 1.4, # allow new object but not over-destructive
            "blend_alpha": 0.75,
        },

        "3": {
            "steps": 55,
            "split_ratio": 0.5,
            "cfg1_text": 7.0, "cfg1_image": 1.7,
            "cfg2_text": 9.2, "cfg2_image": 1.2, # very strong late text push to delete
            "blend_alpha": 0.60,
        },

        "4": {
            "steps": 45,
            "split_ratio": 0.6,
            "cfg1_text": 7.5, "cfg1_image": 1.7,
            "cfg2_text": 8.8, "cfg2_image": 1.3,
            "blend_alpha": 0.55,
        },

        "5": {
            "steps": 50,
            "split_ratio": 0.6,
            "cfg1_text": 7.8, "cfg1_image": 1.6,
            "cfg2_text": 9.0, "cfg2_image": 1.3,
            "blend_alpha": 0.60,
        },

        # v1 had CLIP drop → slightly more text later
        "6": {
            "steps": 45,
            "split_ratio": 0.6,
            "cfg1_text": 7.2, "cfg1_image": 2.0,
            "cfg2_text": 9.0, "cfg2_image": 1.4,
            "blend_alpha": 0.50,
        },

        "7": {
            "steps": 45,
            "split_ratio": 0.6,
            "cfg1_text": 7.5, "cfg1_image": 1.8,
            "cfg2_text": 9.0, "cfg2_image": 1.4,
            "blend_alpha": 0.55,
        },

        # CLIP drop → more late text, but strong early structure
        "8": {
            "steps": 50,
            "split_ratio": 0.55,
            "cfg1_text": 7.0, "cfg1_image": 2.1,
            "cfg2_text": 8.8, "cfg2_image": 1.5,
            "blend_alpha": 0.70,
        },

        # v1 had SSIM/LPIPS worse → soften edit but keep style change
        "9": {
            "steps": 55,
            "split_ratio": 0.65,
            "cfg1_text": 7.0, "cfg1_image": 1.9,
            "cfg2_text": 8.7, "cfg2_image": 1.4,
            "blend_alpha": 0.65,
        },}

    @classmethod
    def get_config(
        cls, editing_type_id, base_steps=50, base_split_ratio=0.6, base_cfg1_text=7.5,
        base_cfg1_image=1.5, base_cfg2_text=8.5, base_cfg2_image=1.3, base_blend=0.7,):
        tid = str(editing_type_id)
        return cls.EDITING_TYPE_CONFIGS.get(
            tid,{
                "steps": base_steps,
                "split_ratio": base_split_ratio,
                "cfg1_text": base_cfg1_text,
                "cfg1_image": base_cfg1_image,
                "cfg2_text": base_cfg2_text,
                "cfg2_image": base_cfg2_image,
                "blend_alpha": base_blend,
            },)

def edit_instruct_pix2pix_improved2(
    model, input_path, edit_instruction, editing_type_id, resolution=512, seed=42,):
    """
    Improved v2 InstructPix2Pix:

    - Per-type 2-stage CFG schedule:
        Stage 1: stronger image-guidance, preserves structure / identity.
        Stage 2: stronger text-guidance, enforces requested semantic change.
    - Per-type blend_alpha in pixel space:
        x_final = (1 - alpha) * original + alpha * edited.
    - Still uses the same baseline InstructPix2Pix checkpoint.
    """

    # Per-type hyperparameters
    cfg = AdaptiveEditConfigV2.get_config(editing_type_id)
    steps = cfg["steps"]
    split_ratio = cfg["split_ratio"]
    cfg1_text = cfg["cfg1_text"]
    cfg1_image = cfg["cfg1_image"]
    cfg2_text = cfg["cfg2_text"]
    cfg2_image = cfg["cfg2_image"]
    blend_alpha = cfg["blend_alpha"]

    # Wrapping model with K-diffusion denoisers
    model_wrap = K.external.CompVisDenoiser(model)
    model_wrap_cfg = CFGDenoiser(model_wrap)

    # Conditioning and image preprocessing
    null_token = model.get_learned_conditioning([""])
    input_image_numpy = Image.open(input_path).convert("RGB")

    # Resizing to multiple of 64, preserving aspect ratio (same as baseline)
    width, height = input_image_numpy.size
    factor = resolution / max(width, height)
    factor = math.ceil(min(width, height) * factor / 64) * 64 / min(width, height)
    width = int((width * factor) // 64) * 64
    height = int((height * factor) // 64) * 64
    input_image = ImageOps.fit(
        input_image_numpy, (width, height), method=Image.Resampling.LANCZOS)

    device = model.device

    with torch.no_grad(), autocast("cuda"):
        cond = {}
        cond["c_crossattn"] = [model.get_learned_conditioning([edit_instruction])]

        # Image -> tensor in [-1, 1], CHW, batch, on model device
        input_image_tensor = 2 * torch.tensor(np.array(input_image)).float() / 255 - 1
        input_image_tensor = rearrange(input_image_tensor, "h w c -> 1 c h w").to(device)
        input_image_tensor = input_image_tensor.half()

        # Keeping [0,1] version for blending
        input_image_01 = torch.clamp(
            (input_image_tensor + 1.0) / 2.0, min=0.0, max=1.0)

        # Encoding to latent
        encoded = model.encode_first_stage(input_image_tensor).mode()
        cond["c_concat"] = [encoded]

        uncond = {}
        uncond["c_crossattn"] = [null_token]
        uncond["c_concat"] = [torch.zeros_like(encoded)]

        # Building sigma schedule and splitting into two stages
        sigmas_full = model_wrap.get_sigmas(steps).to(device)
        # Making sure split index is valid
        split_idx = int(split_ratio * (steps - 1))
        split_idx = max(1, min(steps - 2, split_idx))

        sigmas1 = sigmas_full[: split_idx + 1] # stage 1
        sigmas2 = sigmas_full[split_idx:] # stage 2 (continues from stage 1)

        # Stage 1: structure-preserving (higher image CFG)
        extra_args1 = {
            "cond": cond,
            "uncond": uncond,
            "text_cfg_scale": cfg1_text,
            "image_cfg_scale": cfg1_image,}

        # Stage 2: semantics-focused (higher text CFG)
        extra_args2 = {
            "cond": cond,
            "uncond": uncond,
            "text_cfg_scale": cfg2_text,
            "image_cfg_scale": cfg2_image,}

        # Sampling with 2-stage schedule
        torch.manual_seed(seed)
        z = torch.randn_like(encoded) * sigmas1[0]
        z = K.sampling.sample_euler_ancestral(
            model_wrap_cfg, z, sigmas1, extra_args=extra_args1)
        # Continue from last sigma into the remainder with different CFG
        z = K.sampling.sample_euler_ancestral(
            model_wrap_cfg, z, sigmas2, extra_args=extra_args2)

        # Decoding edited latent to image in [-1, 1]
        x_edit = model.decode_first_stage(z)

        # Converting edited to [0,1]
        x_edit_01 = torch.clamp((x_edit + 1.0) / 2.0, min=0.0, max=1.0)

        # Type-dependent blending in pixel space
        x_final_01 = (1.0 - blend_alpha) * input_image_01 + blend_alpha * x_edit_01

        x = 255.0 * rearrange(x_final_01, "1 c h w -> h w c")
        edited_image = Image.fromarray(x.type(torch.uint8).cpu().numpy())

    return edited_image

print("Improved v2 editing function defined!")

# Evaluating Improved v2 InstructPix2Pix

improved2_output_dir = "/content/output/improved2_instructpix2pix"
os.makedirs(improved2_output_dir, exist_ok=True)

improved2_results = []

print("Starting improved v2 evaluation...")
for idx, (key, item) in enumerate(tqdm(editing_instructions.items(), desc="Improved v2")):
    try:
        image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        editing_instruction = item["editing_instruction"]
        editing_type_id = item["editing_type_id"]

        output_path = os.path.join(improved2_output_dir, item["image_path"])
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # Using the new v2 editing function
        edited_image = edit_instruct_pix2pix_improved2(
            baseline_model, image_path, editing_instruction,
            editing_type_id, resolution=512, seed=42,)
        edited_image.save(output_path)

        improved2_results.append({
            "key": key,
            "image_path": item["image_path"],
            "editing_type_id": editing_type_id,
            "output_path": output_path,})

        if (idx + 1) % 50 == 0:
            print(f"Processed {idx + 1} images...")
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error processing {key}: {str(e)}")
        continue

print(f"Improved v2 evaluation complete! Processed {len(improved2_results)} images.")

# Computing metrics for improved v2 method
improved2_metrics = []

print("Computing Improved v2 metrics...")
for item in tqdm(improved2_results, desc="Improved v2 Metrics"):
    try:
        key = item["key"]
        editing_info = editing_instructions[key]
        src_image_path = os.path.join(
            pie_bench_path, "annotation_images", item["image_path"])
        src_image = Image.open(src_image_path).convert("RGB")
        tgt_image_path = item["output_path"]
        tgt_image = Image.open(tgt_image_path).convert("RGB")
        if tgt_image.size[0] != tgt_image.size[1]:
            tgt_image = tgt_image.crop(
                (tgt_image.size[0] - 512, tgt_image.size[1] - 512,
                 tgt_image.size[0], tgt_image.size[1]))
        ssim = metrics_calc.calculate_ssim(src_image, tgt_image)
        lpips = metrics_calc.calculate_lpips(src_image, tgt_image)
        clip_sim = metrics_calc.calculate_clip_similarity(
            tgt_image, editing_info["editing_prompt"].replace("[", "").replace("]", ""))
        improved2_metrics.append({
            "key": key,
            "editing_type_id": item["editing_type_id"],
            "ssim": ssim,
            "lpips": lpips,
            "clip_similarity": clip_sim,})

    except Exception as e:
        print(f"Error computing metrics for {item['key']}: {str(e)}")
        continue

print(f"Improved v2 metrics computed for {len(improved2_metrics)} images.")

# Aggregating metrics
def aggregate_metrics(metrics_list, method_name):
    overall = {
        "method": method_name, "num_images": len(metrics_list),
        "ssim_mean": np.mean([m["ssim"] for m in metrics_list]),
        "ssim_std": np.std([m["ssim"] for m in metrics_list]),
        "lpips_mean": np.mean([m["lpips"] for m in metrics_list]),
        "lpips_std": np.std([m["lpips"] for m in metrics_list]),
        "clip_similarity_mean": np.mean([m["clip_similarity"] for m in metrics_list]),
        "clip_similarity_std": np.std([m["clip_similarity"] for m in metrics_list]),}
    by_type = defaultdict(list)
    for m in metrics_list:
        by_type[m["editing_type_id"]].append(m)
    type_metrics = {}
    for type_id, type_metrics_list in by_type.items():
        type_metrics[type_id] = {
            "type_name": EDITING_TYPE_NAMES.get(type_id, "unknown"),
            "num_images": len(type_metrics_list),
            "ssim_mean": np.mean([m["ssim"] for m in type_metrics_list]),
            "ssim_std": np.std([m["ssim"] for m in type_metrics_list]),
            "lpips_mean": np.mean([m["lpips"] for m in type_metrics_list]),
            "lpips_std": np.std([m["lpips"] for m in type_metrics_list]),
            "clip_similarity_mean": np.mean([m["clip_similarity"] for m in type_metrics_list]),
            "clip_similarity_std": np.std([m["clip_similarity"] for m in type_metrics_list]),}
    return overall, type_metrics

baseline_overall, baseline_by_type = aggregate_metrics(baseline_metrics, "Baseline InstructPix2Pix")
improved2_overall, improved2_by_type = aggregate_metrics(improved2_metrics, "Improved v2 InstructPix2Pix")

print("Metrics aggregated (v2)!")

print("\n" + "=" * 60)
print("RESULTS BY EDITING TYPE (v2)")
print("=" * 60)

common_type_ids = sorted(set(baseline_by_type.keys()) & set(improved2_by_type.keys()))

for type_id in common_type_ids:
    type_name = baseline_by_type[type_id]["type_name"]
    print(f"\n{type_name} (Type {type_id}):")
    print(f"  Baseline    - SSIM: {baseline_by_type[type_id]['ssim_mean']:.4f}, "
          f"LPIPS: {baseline_by_type[type_id]['lpips_mean']:.4f}, "
          f"CLIP: {baseline_by_type[type_id]['clip_similarity_mean']:.4f}")
    print(f"  Improved_v2 - SSIM: {improved2_by_type[type_id]['ssim_mean']:.4f}, "
          f"LPIPS: {improved2_by_type[type_id]['lpips_mean']:.4f}, "
          f"CLIP: {improved2_by_type[type_id]['clip_similarity_mean']:.4f}")

# Printing overall results (Baseline vs Improved v2)
print("=" * 40)
print("OVERALL RESULTS (BASELINE VS IMPROVED v2)")
print("=" * 40)

print(f"\nBaseline InstructPix2Pix:")
print(f"  SSIM: {baseline_overall['ssim_mean']:.4f} ± {baseline_overall['ssim_std']:.4f}")
print(f"  LPIPS: {baseline_overall['lpips_mean']:.4f} ± {baseline_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {baseline_overall['clip_similarity_mean']:.4f} ± {baseline_overall['clip_similarity_std']:.4f}")

print(f"\nImproved v2 InstructPix2Pix:")
print(f"  SSIM: {improved2_overall['ssim_mean']:.4f} ± {improved2_overall['ssim_std']:.4f}")
print(f"  LPIPS: {improved2_overall['lpips_mean']:.4f} ± {improved2_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {improved2_overall['clip_similarity_mean']:.4f} ± {improved2_overall['clip_similarity_std']:.4f}")

print(f"\nImprovements (Improved v2 − Baseline):")
print(f"  SSIM: {improved2_overall['ssim_mean'] - baseline_overall['ssim_mean']:+.4f} (Higher means improved)")
print(f"  LPIPS: {improved2_overall['lpips_mean'] - baseline_overall['lpips_mean']:+.4f} (Lower means improved)")
print(f"  CLIP Similarity: {improved2_overall['clip_similarity_mean'] - baseline_overall['clip_similarity_mean']:+.4f} (Higher Better)")

print("Baseline results:", len(baseline_results))
print("Improved results:", len(improved_results))
print("Improved_v2 results:", len(improved2_results))
print("Baseline metrics:", len(baseline_metrics))
print("Improved metrics:", len(improved_metrics))
print("Improved_v2 metrics:", len(improved2_metrics))

from PIL import Image
import numpy as np

k = baseline_results[0]["key"]
b_path = [r["output_path"] for r in baseline_results if r["key"] == k][0]
i_path = [r["output_path"] for r in improved2_results if r["key"] == k][0]

b_img = np.array(Image.open(b_path).convert("RGB"))
i_img = np.array(Image.open(i_path).convert("RGB"))

print("Shape baseline:", b_img.shape, "Shape improved:", i_img.shape)
print("Max abs diff:", np.abs(b_img.astype(np.int16) - i_img.astype(np.int16)).max())

results2_data = []

# Overall results (v2)
results2_data.append({
    "Method": "Baseline InstructPix2Pix", "Type": "Overall",
    "SSIM_Mean": baseline_overall["ssim_mean"], "SSIM_Std": baseline_overall["ssim_std"],
    "LPIPS_Mean": baseline_overall["lpips_mean"], "LPIPS_Std": baseline_overall["lpips_std"],
    "CLIP_Mean": baseline_overall["clip_similarity_mean"], "CLIP_Std": baseline_overall["clip_similarity_std"],
    "Num_Images": baseline_overall["num_images"],
})

results2_data.append({
    "Method": "Improved v2 InstructPix2Pix", "Type": "Overall",
    "SSIM_Mean": improved2_overall["ssim_mean"], "SSIM_Std": improved2_overall["ssim_std"],
    "LPIPS_Mean": improved2_overall["lpips_mean"], "LPIPS_Std": improved2_overall["lpips_std"],
    "CLIP_Mean": improved2_overall["clip_similarity_mean"], "CLIP_Std": improved2_overall["clip_similarity_std"],
    "Num_Images": improved2_overall["num_images"],
})

# Results by type (v2)
for type_id in sorted(baseline_by_type.keys()):
    type_name = baseline_by_type[type_id]["type_name"]

    # baseline row
    results2_data.append({
        "Method": "Baseline InstructPix2Pix", "Type": type_name,
        "SSIM_Mean": baseline_by_type[type_id]["ssim_mean"], "SSIM_Std": baseline_by_type[type_id]["ssim_std"],
        "LPIPS_Mean": baseline_by_type[type_id]["lpips_mean"], "LPIPS_Std": baseline_by_type[type_id]["lpips_std"],
        "CLIP_Mean": baseline_by_type[type_id]["clip_similarity_mean"], "CLIP_Std": baseline_by_type[type_id]["clip_similarity_std"],
        "Num_Images": baseline_by_type[type_id]["num_images"],
    })

    # improved v2 row
    results2_data.append({
        "Method": "Improved v2 InstructPix2Pix", "Type": type_name,
        "SSIM_Mean": improved2_by_type[type_id]["ssim_mean"], "SSIM_Std": improved2_by_type[type_id]["ssim_std"],
        "LPIPS_Mean": improved2_by_type[type_id]["lpips_mean"], "LPIPS_Std": improved2_by_type[type_id]["lpips_std"],
        "CLIP_Mean": improved2_by_type[type_id]["clip_similarity_mean"], "CLIP_Std": improved2_by_type[type_id]["clip_similarity_std"],
        "Num_Images": improved2_by_type[type_id]["num_images"],
    })

df_results2 = pd.DataFrame(results2_data)
results_path2 = "/content/results/evaluation_results_v2.csv"
df_results2.to_csv(results_path2, index=False)

print(f"\nResults (v2) saved to {results_path2}")
print("\nResults DataFrame (v2):")
print(df_results2.to_string())

df_types2 = df_results2[df_results2["Type"] != "Overall"].copy()

# Separating baseline vs improved v2 and index by type
baseline_v2_df = df_types2[df_types2["Method"] == "Baseline InstructPix2Pix"].set_index("Type")
improved2_df   = df_types2[df_types2["Method"] == "Improved v2 InstructPix2Pix"].set_index("Type")

# Editing types as ordered list
types_v2 = baseline_v2_df.index.tolist()
x_v2 = np.arange(len(types_v2))

print("Types (v2):", types_v2)
print("Baseline rows (v2):", len(baseline_v2_df), "Improved v2 rows:", len(improved2_df))

fig, axes = plt.subplots(1,3,figsize=(18,4),constrained_layout=False)

metrics = [
    ("SSIM_Mean", "SSIM"),
    ("LPIPS_Mean", "LPIPS (↓ better)"),
    ("CLIP_Mean", "CLIP Similarity"),]

width = 0.28
for ax, (metric_col, title) in zip(axes, metrics):
    baseline_vals = baseline_v2_df[metric_col].values
    improved_vals = improved2_df[metric_col].values

    ax.bar(x_v2 - width/2, baseline_vals, width, label="Baseline", alpha=0.8)
    ax.bar(x_v2 + width/2, improved_vals, width, label="Improved v2", alpha=0.8)

    ax.set_xticks(x_v2)
    ax.set_xticklabels(types_v2, rotation=35, ha="right", fontsize=8)
    ax.set_title(title, fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.tick_params(axis="y", labelsize=8)

axes[0].set_ylabel("Score", fontsize=10)
axes[0].legend(loc="upper left", fontsize=8)

fig.suptitle("Baseline vs Improved_v2 by Editing Type", fontsize=14, y=0.98)
fig.tight_layout(rect=[0, 0, 1, 0.99])

plt.savefig("/content/results/metrics_comparison1_v2.png", dpi=300, bbox_inches="tight")
plt.show()

print("Saved bar plot to /content/results/metrics_comparison1_v2.png")

fig, axes = plt.subplots(1,3,figsize=(18,4),constrained_layout=False)

metrics = [
    ("SSIM_Mean", "ΔSSIM (Improved v2 − Baseline)"),
    ("LPIPS_Mean", "ΔLPIPS (Improved v2 − Baseline, ↓ better)"),
    ("CLIP_Mean", "ΔCLIP (Improved v2 − Baseline)"),]

for ax, (metric_col, title) in zip(axes, metrics):
    delta = improved2_df[metric_col] - baseline_v2_df[metric_col]

    ax.axhline(0.0, linestyle="--", linewidth=1, color="gray")
    ax.bar(x_v2, delta.values, width=0.5, alpha=0.85, color="seagreen")

    if len(delta) > 0:
        d_min = float(np.nanmin(delta.values))
        d_max = float(np.nanmax(delta.values))
        if d_min == d_max:
            d_min -= 0.1
            d_max += 0.1
        margin = 0.1 * (d_max - d_min)
        y_min = d_min - margin
        y_max = d_max + margin

        y_min = min(y_min, -0.05)
        y_max = max(y_max,  0.05)
        ax.set_ylim(y_min, y_max)

    ax.set_xticks(x_v2)
    ax.set_xticklabels(types_v2, rotation=35, ha="right", fontsize=8)
    ax.set_title(title, fontsize=11)
    ax.grid(True, axis="y", alpha=0.3)
    ax.tick_params(axis="y", labelsize=8)

    y_min, y_max = ax.get_ylim()
    offset = 0.02 * (y_max - y_min)
    for xi, d in zip(x_v2, delta.values):
        if np.isnan(d):
            continue
        va = "bottom" if d >= 0 else "top"
        y_text = d + (offset if d >= 0 else -offset)
        ax.text(xi, y_text, f"{d:.3f}", ha="center", va=va, fontsize=7)

fig.suptitle("Per-Type Metric Improvements (Improved_v2 - Baseline)",fontsize=14,y=0.98)
fig.tight_layout(rect=[0, 0, 1, 0.99])

plt.savefig("/content/results/metrics_deltas_v2.png", dpi=300, bbox_inches="tight")
plt.show()

print("Saved delta plot to /content/results/metrics_deltas_v2.png")

overall_df2 = df_results2[df_results2["Type"] == "Overall"].set_index("Method")
baseline_overall = overall_df2.loc["Baseline InstructPix2Pix"]
improved2_overall = overall_df2.loc["Improved v2 InstructPix2Pix"]

print("Baseline overall:")
print("  SSIM  =", baseline_overall["SSIM_Mean"])
print("  LPIPS =", baseline_overall["LPIPS_Mean"])
print("  CLIP  =", baseline_overall["CLIP_Mean"])
print("\nImproved v2 overall:")
print("  SSIM  =", improved2_overall["SSIM_Mean"])
print("  LPIPS =", improved2_overall["LPIPS_Mean"])
print("  CLIP  =", improved2_overall["CLIP_Mean"])

baseline_scores = np.array([
    baseline_overall["SSIM_Mean"], # SSIM
    1.0 - baseline_overall["LPIPS_Mean"], # 1 - LPIPS
    baseline_overall["CLIP_Mean"] / 30.0, # CLIP scaled
])

improved2_scores = np.array([
    improved2_overall["SSIM_Mean"],
    1.0 - improved2_overall["LPIPS_Mean"],
    improved2_overall["CLIP_Mean"] / 30.0,])

labels = ["SSIM", "1 − LPIPS", "CLIP / 30"]

angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False)
angles = np.concatenate([angles, [angles[0]]])
baseline_scores = np.concatenate([baseline_scores, [baseline_scores[0]]])
improved2_scores = np.concatenate([improved2_scores, [improved2_scores[0]]])

all_scores = np.concatenate([baseline_scores, improved2_scores])
s_min = float(all_scores.min())
s_max = float(all_scores.max())
margin = 0.05 * (s_max - s_min)

r_min = max(0.0, s_min - margin)
r_max = min(1.0, s_max + margin)

print("\nRadar radial limits:", r_min, "to", r_max)

fig = plt.figure(figsize=(5, 5))
ax = fig.add_subplot(111, polar=True)

ax.plot(angles, baseline_scores, marker="o", label="Baseline", linewidth=2)
ax.fill(angles, baseline_scores, alpha=0.25)

ax.plot(angles, improved2_scores, marker="o", label="Improved v2", linewidth=2)
ax.fill(angles, improved2_scores, alpha=0.25)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(labels, fontsize=10)

ax.set_ylim(r_min, r_max)

ax.set_title("Overall Metrics (Normalized Radar Plot v2)", fontsize=14, y=1.12)
ax.legend(loc="upper right", bbox_to_anchor=(1.25, 1.1), fontsize=9)

plt.tight_layout()
plt.savefig("/content/results/overall_radar_v2.png", dpi=300, bbox_inches="tight")
plt.show()

print("Saved radar plot to /content/results/overall_radar_v2.png")

"""## Appendix - Improved v4 InstructPix2Pix"""

# Improved v4 editing function with Enhanced Text Encoder + AdaptiveEditConfigV3
def edit_instruct_pix2pix_improved4(model, input_path: str, edit_instruction: str,
    editing_type_id: str, resolution: int = 512, seed: int = 42,):
    """
    Improved v4 InstructPix2Pix with Enhanced Text Encoder:
    - Uses AdaptiveEditConfigV3 per edit type:
        * steps: total diffusion steps
        * split_ratio: fraction of steps for stage 1 (structure-preserving)
        * cfg1_text, cfg1_image: CFG scales for stage 1
        * cfg2_text, cfg2_image: CFG scales for stage 2 (semantic stage)
        * blend_alpha: pixel-space blend weight
    - Replaces vanilla text conditioning with fused
      (SD1.5 CLIP + OpenCLIP ViT-H/14) tokens via get_enhanced_conditioning.
    """
    # Per-type hyperparameters from AdaptiveEditConfigV3
    cfg = AdaptiveEditConfigV3.get_config(editing_type_id)
    steps = cfg["steps"]
    split_ratio = cfg["split_ratio"]
    cfg1_text = cfg["cfg1_text"]
    cfg1_image = cfg["cfg1_image"]
    cfg2_text = cfg["cfg2_text"]
    cfg2_image = cfg["cfg2_image"]
    blend_alpha = cfg["blend_alpha"]

    # Wrapping model with K-diffusion + CFG denoiser
    model_wrap = K.external.CompVisDenoiser(model)
    model_wrap_cfg = CFGDenoiser(model_wrap)
    null_token = model.get_learned_conditioning([""])

    # Loading and resizing input image
    input_image = Image.open(input_path).convert("RGB")
    width, height = input_image.size

    short_side = min(width, height)
    if short_side <= 0:
        raise ValueError(f"Invalid image size: {width}x{height}")

    # Scaling so that short side ~ resolution, then snap to 64 grid
    factor = resolution / short_side
    factor = math.ceil(short_side * factor / 64) * 64 / short_side
    width = int((width * factor) // 64) * 64
    height = int((height * factor) // 64) * 64

    input_image = ImageOps.fit(
        input_image, (width, height), method=Image.Resampling.LANCZOS)

    # Diffusion sampling with 2-stage schedule
    with torch.no_grad(), autocast("cuda"):
        # text conditioning: enhanced encoder + fusion
        cond = {}
        cond["c_crossattn"] = [
            get_enhanced_conditioning(model,
                edit_instruction=edit_instruction,
                editing_type_id=editing_type_id,)]

        # Image -> tensor in [-1, 1], CHW, batch, fp16 on model.device
        input_image_tensor = 2 * torch.tensor(np.array(input_image)).float() / 255.0 - 1.0
        input_image_tensor = rearrange(input_image_tensor, "h w c -> 1 c h w").to(
            model.device)
        input_image_tensor = input_image_tensor.half()

        # Keeping [0, 1] version for blending later
        input_image_01 = torch.clamp(
            (input_image_tensor + 1.0) / 2.0, min=0.0, max=1.0)

        # Encoding image to latent
        encoded = model.encode_first_stage(input_image_tensor).mode()
        cond["c_concat"] = [encoded]

        # Unconditional branch
        uncond = {
            "c_crossattn": [null_token],
            "c_concat": [torch.zeros_like(encoded)],}

        # 2-stage sigma schedule
        sigmas = model_wrap.get_sigmas(steps).to(model.device)

        # Ensuring at least 1 step in each stage
        split_idx = max(1, min(len(sigmas) - 1, int(len(sigmas) * split_ratio)))
        sigmas1 = sigmas[:split_idx]
        sigmas2 = sigmas[split_idx - 1 :]

        extra_args1 = {
            "cond": cond,
            "uncond": uncond,
            "text_cfg_scale": cfg1_text,
            "image_cfg_scale": cfg1_image,}

        extra_args2 = {
            "cond": cond,
            "uncond": uncond,
            "text_cfg_scale": cfg2_text,
            "image_cfg_scale": cfg2_image,}

        # Stage 1 + Stage 2 sampling
        torch.manual_seed(seed)
        z = torch.randn_like(encoded) * sigmas1[0]

        # Stage 1: structure-preserving (stronger image CFG)
        z = K.sampling.sample_euler_ancestral(
            model_wrap_cfg, z, sigmas1, extra_args=extra_args1)

        # Stage 2: semantic refinement (stronger text CFG)
        z = K.sampling.sample_euler_ancestral(
            model_wrap_cfg, z, sigmas2, extra_args=extra_args2)

        # Decoding latent to image in [-1, 1]
        x_edit = model.decode_first_stage(z)

        # Converting edited to [0, 1]
        x_edit_01 = torch.clamp((x_edit + 1.0) / 2.0, min=0.0, max=1.0)

        # Type-dependent blending in pixel space
        x_final_01 = (1.0 - blend_alpha) * input_image_01 + blend_alpha * x_edit_01

        x = 255.0 * rearrange(x_final_01, "1 c h w -> h w c")
        edited_image = Image.fromarray(x.type(torch.uint8).cpu().numpy())

    return edited_image

print("Enhanced-text Improved v4 editing function defined!")

improved4_output_dir = "/content/output/improved_instructpix2pix_v4"
os.makedirs(improved4_output_dir, exist_ok=True)

improved4_results = []

print("Starting improved v4 evaluation...")
for idx, (key, item) in enumerate(tqdm(editing_instructions.items(), desc="Improved v4")):
    try:
        image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        editing_instruction = item["editing_instruction"]
        editing_type_id = item["editing_type_id"]

        output_path = os.path.join(improved4_output_dir, item["image_path"])
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # v4 editing function
        edited_image = edit_instruct_pix2pix_improved4(
            baseline_model, image_path, editing_instruction,
            editing_type_id, resolution=512, seed=42,)
        edited_image.save(output_path)

        improved4_results.append({
            "key": key,
            "image_path": item["image_path"],
            "editing_type_id": editing_type_id,
            "output_path": output_path,})

        if (idx + 1) % 50 == 0:
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error processing {key}: {e}")
        continue

print(f"Improved v4 evaluation complete! Processed {len(improved4_results)} images.")

# Computing metrics for improved v4
improved4_metrics = []

print("Computing Improved v4 metrics...")
for item in tqdm(improved4_results, desc="Improved v4 Metrics"):
    try:
        key = item["key"]
        editing_info = editing_instructions[key]
        src_image_path = os.path.join(pie_bench_path, "annotation_images", item["image_path"])
        src_image = Image.open(src_image_path).convert("RGB")
        tgt_image_path = item["output_path"]
        tgt_image = Image.open(tgt_image_path).convert("RGB")
        if tgt_image.size[0] != tgt_image.size[1]:
            tgt_image = tgt_image.crop(
                (tgt_image.size[0] - 512, tgt_image.size[1] - 512,
                 tgt_image.size[0], tgt_image.size[1]))
        ssim = metrics_calc.calculate_ssim(src_image, tgt_image)
        lpips = metrics_calc.calculate_lpips(src_image, tgt_image)
        clip_sim = metrics_calc.calculate_clip_similarity(
            tgt_image, editing_info["editing_prompt"].replace("[", "").replace("]", ""))

        improved4_metrics.append({
            "key": key, "editing_type_id": item["editing_type_id"], "ssim": ssim,
            "lpips": lpips, "clip_similarity": clip_sim,})

    except Exception as e:
        print(f"Error computing metrics for {item['key']}: {str(e)}")
        continue

print(f"Improved v4 metrics computed for {len(improved4_metrics)} images.")

# Aggregating metrics
def aggregate_metrics(metrics_list, method_name):
    overall = {
        "method": method_name,
        "num_images": len(metrics_list),
        "ssim_mean": np.mean([m["ssim"] for m in metrics_list]),
        "ssim_std": np.std([m["ssim"] for m in metrics_list]),
        "lpips_mean": np.mean([m["lpips"] for m in metrics_list]),
        "lpips_std": np.std([m["lpips"] for m in metrics_list]),
        "clip_similarity_mean": np.mean([m["clip_similarity"] for m in metrics_list]),
        "clip_similarity_std": np.std([m["clip_similarity"] for m in metrics_list]),}
    by_type = defaultdict(list)
    for m in metrics_list:
        by_type[m["editing_type_id"]].append(m)
    type_metrics = {}
    for type_id, type_metrics_list in by_type.items():
        type_metrics[type_id] = {
            "type_name": EDITING_TYPE_NAMES.get(type_id, "unknown"),
            "num_images": len(type_metrics_list),
            "ssim_mean": np.mean([m["ssim"] for m in type_metrics_list]),
            "ssim_std": np.std([m["ssim"] for m in type_metrics_list]),
            "lpips_mean": np.mean([m["lpips"] for m in type_metrics_list]),
            "lpips_std": np.std([m["lpips"] for m in type_metrics_list]),
            "clip_similarity_mean": np.mean([m["clip_similarity"] for m in type_metrics_list]),
            "clip_similarity_std": np.std([m["clip_similarity"] for m in type_metrics_list]),}
    return overall, type_metrics

# Baseline + Improved v4
baseline_overall, baseline_by_type = aggregate_metrics(baseline_metrics, "Baseline InstructPix2Pix")
improved4_overall, improved4_by_type = aggregate_metrics(improved4_metrics, "Improved v4 InstructPix2Pix")

print("Metrics aggregated for baseline and improved v4!")

print("Baseline results:", len(baseline_results))
print("Improved_v4 results:", len(improved4_results))
print("Baseline metrics:", len(baseline_metrics))
print("Improved_v4 metrics:", len(improved4_metrics))

# Improved v4 version
from PIL import Image
import numpy as np

k = baseline_results[0]["key"]
b_path = [r["output_path"] for r in baseline_results if r["key"] == k][0]
i_path = [r["output_path"] for r in improved4_results if r["key"] == k][0]

b_img = np.array(Image.open(b_path).convert("RGB"))
i_img = np.array(Image.open(i_path).convert("RGB"))

print("Shape baseline:", b_img.shape, "Shape improved:", i_img.shape)
print("Max abs diff:", np.abs(b_img.astype(np.int16) - i_img.astype(np.int16)).max())

print("\n" + "=" * 60)
print("RESULTS BY EDITING TYPE (v4)")
print("=" * 60)

common_type_ids = sorted(set(baseline_by_type.keys()) & set(improved4_by_type.keys()))

for type_id in common_type_ids:
    type_name = baseline_by_type[type_id]["type_name"]
    print(f"\n{type_name} (Type {type_id}):")
    print(f"  Baseline    - SSIM: {baseline_by_type[type_id]['ssim_mean']:.4f}, "
        f"LPIPS: {baseline_by_type[type_id]['lpips_mean']:.4f}, "
        f"CLIP: {baseline_by_type[type_id]['clip_similarity_mean']:.4f}")
    print(f"  Improved_v4 - SSIM: {improved4_by_type[type_id]['ssim_mean']:.4f}, "
        f"LPIPS: {improved4_by_type[type_id]['lpips_mean']:.4f}, "
        f"CLIP: {improved4_by_type[type_id]['clip_similarity_mean']:.4f}")

# Printing overall results
print("=" * 40)
print("OVERALL RESULTS (BASELINE VS IMPROVED v4)")
print("=" * 40)

print(f"\nBaseline InstructPix2Pix:")
print(f"  SSIM: {baseline_overall['ssim_mean']:.4f} ± {baseline_overall['ssim_std']:.4f}")
print(f"  LPIPS: {baseline_overall['lpips_mean']:.4f} ± {baseline_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {baseline_overall['clip_similarity_mean']:.4f} ± {baseline_overall['clip_similarity_std']:.4f}")

print(f"\nImproved v4 InstructPix2Pix:")
print(f"  SSIM: {improved4_overall['ssim_mean']:.4f} ± {improved4_overall['ssim_std']:.4f}")
print(f"  LPIPS: {improved4_overall['lpips_mean']:.4f} ± {improved4_overall['lpips_std']:.4f}")
print(f"  CLIP Similarity: {improved4_overall['clip_similarity_mean']:.4f} ± {improved4_overall['clip_similarity_std']:.4f}")

print(f"\nImprovements (Improved v4 − Baseline):")
print(f"  SSIM: {improved4_overall['ssim_mean'] - baseline_overall['ssim_mean']:+.4f} (Higher means improved)")
print(f"  LPIPS: {improved4_overall['lpips_mean'] - baseline_overall['lpips_mean']:+.4f} (Lower means improved)")
print(f"  CLIP Similarity: {improved4_overall['clip_similarity_mean'] - baseline_overall['clip_similarity_mean']:+.4f} (High Better)")